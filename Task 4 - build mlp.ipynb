{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee981db4",
   "metadata": {},
   "source": [
    "# Important links\n",
    "\n",
    "Combining two pretrained separate models side by side: https://discuss.pytorch.org/t/combining-trained-models-in-pytorch/28383/7?u=nagabhushansn95\n",
    "\n",
    "combine two models side by side and train (using graph neural networks): https://datascience.stackexchange.com/questions/117110/is-it-possible-to-combine-models-in-pytorch-and-pytorch-geometric\n",
    "\n",
    "Combining two separate models one after the other: https://stackoverflow.com/questions/65216411/how-to-concatenate-2-pytorch-models-and-make-the-first-one-non-trainable-in-pyto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c2e35ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'loss_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mloss_fn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'loss_fn'"
     ]
    }
   ],
   "source": [
    "import loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e7e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to do: train one to get scale (std)\n",
    "    get the intended std outputs\n",
    "    map each to an input\n",
    "    train.\n",
    "\n",
    "train both and get the output into a matrix.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522b24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PHONEMES =['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh', 'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl', 'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e31d8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        super(MLP, self).__init__()\n",
    "        self.sequential = nn.Sequential()\n",
    "        self.sequential.add_module(\"dense1\", nn.Linear(input_size, output_size))\n",
    "        self.sequential.add_module(\"act1\", activation())\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.sequential(x)\n",
    "#         print(h.shape)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba5e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = \"saved_variables/average_and_std_phoneme_feature/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d733345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "phonemes = []\n",
    "scales = []\n",
    "shifts = []\n",
    "\n",
    "for indx in range(len(ALL_PHONEMES)):\n",
    "    phoneme_tensor = np.zeros(len(ALL_PHONEMES))\n",
    "    phoneme_tensor[indx] = 1\n",
    "    scale_file = predictions_folder+ALL_PHONEMES[indx]+\"_std.txt\"\n",
    "    shift_file = predictions_folder+ALL_PHONEMES[indx]+\"_average.txt\"\n",
    "    if os.path.isfile(scale_file):\n",
    "        scales.append(np.loadtxt(scale_file)[:65])\n",
    "        shifts.append(np.loadtxt(shift_file)[:65])\n",
    "        phonemes.append(phoneme_tensor)\n",
    "#         print(ALL_PHONEMES[indx])\n",
    "        \n",
    "phonemes = np.array(phonemes)\n",
    "scales = np.array(scales)\n",
    "shifts = np.array(shifts)\n",
    "\n",
    "phonemes = torch.from_numpy(phonemes).float()\n",
    "scales = torch.from_numpy(scales).float()\n",
    "shifts = torch.from_numpy(shifts).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_model = MLP(61, 65, nn.ReLU)\n",
    "optimizer_scales = torch.optim.Adam(scale_model.parameters(), lr=0.03)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    scales_pred = scale_model(phonemes)\n",
    "    loss_scale = loss_fn(scales_pred,scales)\n",
    "    losses.append(loss_scale)\n",
    "    print(f\"epoch: {epoch}, loss: {loss_scale}\")\n",
    "    optimizer_scales.zero_grad()\n",
    "    loss_scale.backward()\n",
    "    optimizer_scales.step()\n",
    "#     if len(losses)>10:\n",
    "#         if losses[-1]>=losses[-10] or losses[-1]>=losses[-5]:\n",
    "#             print(\"detecting minimum validation loss. stopping early\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "862e6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: edit this to make relevant to saving both models\n",
    "torch.save({\"model\":scale_model.state_dict(), \"optimizer\":optimizer_scales.state_dict()}, \"scales_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "802fff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 50.96644592285156\n",
      "epoch: 1, loss: 31.800708770751953\n",
      "epoch: 2, loss: 31.497467041015625\n",
      "epoch: 3, loss: 31.215972900390625\n",
      "epoch: 4, loss: 30.937118530273438\n",
      "epoch: 5, loss: 30.660869598388672\n",
      "epoch: 6, loss: 30.387210845947266\n",
      "epoch: 7, loss: 30.11610221862793\n",
      "epoch: 8, loss: 29.847532272338867\n",
      "epoch: 9, loss: 29.58147430419922\n",
      "epoch: 10, loss: 29.317909240722656\n",
      "epoch: 11, loss: 29.05680274963379\n",
      "epoch: 12, loss: 28.79814338684082\n",
      "epoch: 13, loss: 28.541902542114258\n",
      "epoch: 14, loss: 28.28805160522461\n",
      "epoch: 15, loss: 28.036579132080078\n",
      "epoch: 16, loss: 27.787460327148438\n",
      "epoch: 17, loss: 27.540666580200195\n",
      "epoch: 18, loss: 27.296184539794922\n",
      "epoch: 19, loss: 27.053987503051758\n",
      "epoch: 20, loss: 26.814054489135742\n",
      "epoch: 21, loss: 26.576372146606445\n",
      "epoch: 22, loss: 26.340906143188477\n",
      "epoch: 23, loss: 26.10764503479004\n",
      "epoch: 24, loss: 25.876562118530273\n",
      "epoch: 25, loss: 25.64763832092285\n",
      "epoch: 26, loss: 25.42084503173828\n",
      "epoch: 27, loss: 25.19183921813965\n",
      "epoch: 28, loss: 25.609769821166992\n",
      "epoch: 29, loss: 24.903661727905273\n",
      "epoch: 30, loss: 24.683835983276367\n",
      "epoch: 31, loss: 24.466068267822266\n",
      "epoch: 32, loss: 24.250335693359375\n",
      "epoch: 33, loss: 24.03662109375\n",
      "epoch: 34, loss: 23.82490348815918\n",
      "epoch: 35, loss: 23.61516761779785\n",
      "epoch: 36, loss: 23.407390594482422\n",
      "epoch: 37, loss: 23.20155906677246\n",
      "epoch: 38, loss: 22.997650146484375\n",
      "epoch: 39, loss: 22.7956485748291\n",
      "epoch: 40, loss: 22.595535278320312\n",
      "epoch: 41, loss: 22.397294998168945\n",
      "epoch: 42, loss: 22.20090675354004\n",
      "epoch: 43, loss: 22.00635528564453\n",
      "epoch: 44, loss: 21.81361961364746\n",
      "epoch: 45, loss: 21.622690200805664\n",
      "epoch: 46, loss: 21.433544158935547\n",
      "epoch: 47, loss: 21.246166229248047\n",
      "epoch: 48, loss: 21.06053924560547\n",
      "epoch: 49, loss: 20.876651763916016\n",
      "epoch: 50, loss: 20.69447898864746\n",
      "epoch: 51, loss: 20.514013290405273\n",
      "epoch: 52, loss: 20.335229873657227\n",
      "epoch: 53, loss: 20.15812110900879\n",
      "epoch: 54, loss: 19.982667922973633\n",
      "epoch: 55, loss: 19.808855056762695\n",
      "epoch: 56, loss: 19.636667251586914\n",
      "epoch: 57, loss: 19.466087341308594\n",
      "epoch: 58, loss: 19.297103881835938\n",
      "epoch: 59, loss: 19.12969970703125\n",
      "epoch: 60, loss: 18.9638614654541\n",
      "epoch: 61, loss: 18.799570083618164\n",
      "epoch: 62, loss: 18.636817932128906\n",
      "epoch: 63, loss: 18.475587844848633\n",
      "epoch: 64, loss: 18.315860748291016\n",
      "epoch: 65, loss: 18.157629013061523\n",
      "epoch: 66, loss: 18.000877380371094\n",
      "epoch: 67, loss: 17.845590591430664\n",
      "epoch: 68, loss: 17.691753387451172\n",
      "epoch: 69, loss: 17.539356231689453\n",
      "epoch: 70, loss: 17.38838005065918\n",
      "epoch: 71, loss: 17.238821029663086\n",
      "epoch: 72, loss: 17.090656280517578\n",
      "epoch: 73, loss: 16.943876266479492\n",
      "epoch: 74, loss: 16.7984676361084\n",
      "epoch: 75, loss: 16.6544189453125\n",
      "epoch: 76, loss: 16.511714935302734\n",
      "epoch: 77, loss: 16.370346069335938\n",
      "epoch: 78, loss: 16.230300903320312\n",
      "epoch: 79, loss: 16.091562271118164\n",
      "epoch: 80, loss: 15.954118728637695\n",
      "epoch: 81, loss: 15.817961692810059\n",
      "epoch: 82, loss: 15.683076858520508\n",
      "epoch: 83, loss: 15.549452781677246\n",
      "epoch: 84, loss: 15.417076110839844\n",
      "epoch: 85, loss: 15.285937309265137\n",
      "epoch: 86, loss: 15.156023025512695\n",
      "epoch: 87, loss: 15.027324676513672\n",
      "epoch: 88, loss: 14.89982795715332\n",
      "epoch: 89, loss: 14.773523330688477\n",
      "epoch: 90, loss: 14.648398399353027\n",
      "epoch: 91, loss: 14.524441719055176\n",
      "epoch: 92, loss: 14.40164566040039\n",
      "epoch: 93, loss: 14.279994010925293\n",
      "epoch: 94, loss: 14.159481048583984\n",
      "epoch: 95, loss: 14.040093421936035\n",
      "epoch: 96, loss: 13.921820640563965\n",
      "epoch: 97, loss: 13.804654121398926\n",
      "epoch: 98, loss: 13.688581466674805\n",
      "epoch: 99, loss: 13.573593139648438\n",
      "epoch: 100, loss: 13.459680557250977\n",
      "epoch: 101, loss: 13.346831321716309\n",
      "epoch: 102, loss: 13.235034942626953\n",
      "epoch: 103, loss: 13.124285697937012\n",
      "epoch: 104, loss: 13.014571189880371\n",
      "epoch: 105, loss: 12.905879974365234\n",
      "epoch: 106, loss: 12.79820442199707\n",
      "epoch: 107, loss: 12.691532135009766\n",
      "epoch: 108, loss: 12.585860252380371\n",
      "epoch: 109, loss: 12.481173515319824\n",
      "epoch: 110, loss: 12.37746524810791\n",
      "epoch: 111, loss: 12.274725914001465\n",
      "epoch: 112, loss: 12.17294692993164\n",
      "epoch: 113, loss: 12.072117805480957\n",
      "epoch: 114, loss: 11.972229957580566\n",
      "epoch: 115, loss: 11.873274803161621\n",
      "epoch: 116, loss: 11.775245666503906\n",
      "epoch: 117, loss: 11.678130149841309\n",
      "epoch: 118, loss: 11.581923484802246\n",
      "epoch: 119, loss: 11.486613273620605\n",
      "epoch: 120, loss: 11.392195701599121\n",
      "epoch: 121, loss: 11.29865837097168\n",
      "epoch: 122, loss: 11.20599365234375\n",
      "epoch: 123, loss: 11.11419677734375\n",
      "epoch: 124, loss: 11.023255348205566\n",
      "epoch: 125, loss: 10.9331636428833\n",
      "epoch: 126, loss: 10.843914985656738\n",
      "epoch: 127, loss: 10.755496978759766\n",
      "epoch: 128, loss: 10.667905807495117\n",
      "epoch: 129, loss: 10.581130981445312\n",
      "epoch: 130, loss: 10.49516773223877\n",
      "epoch: 131, loss: 10.41000747680664\n",
      "epoch: 132, loss: 10.325642585754395\n",
      "epoch: 133, loss: 10.242064476013184\n",
      "epoch: 134, loss: 10.159266471862793\n",
      "epoch: 135, loss: 10.077241897583008\n",
      "epoch: 136, loss: 9.995983123779297\n",
      "epoch: 137, loss: 9.915483474731445\n",
      "epoch: 138, loss: 9.835734367370605\n",
      "epoch: 139, loss: 9.756730079650879\n",
      "epoch: 140, loss: 9.678462982177734\n",
      "epoch: 141, loss: 9.60092830657959\n",
      "epoch: 142, loss: 9.524115562438965\n",
      "epoch: 143, loss: 9.448019981384277\n",
      "epoch: 144, loss: 9.372635841369629\n",
      "epoch: 145, loss: 9.297954559326172\n",
      "epoch: 146, loss: 9.223969459533691\n",
      "epoch: 147, loss: 9.150675773620605\n",
      "epoch: 148, loss: 9.078065872192383\n",
      "epoch: 149, loss: 9.006134986877441\n",
      "epoch: 150, loss: 8.934873580932617\n",
      "epoch: 151, loss: 8.864277839660645\n",
      "epoch: 152, loss: 8.794341087341309\n",
      "epoch: 153, loss: 8.725055694580078\n",
      "epoch: 154, loss: 8.656418800354004\n",
      "epoch: 155, loss: 8.588421821594238\n",
      "epoch: 156, loss: 8.521059036254883\n",
      "epoch: 157, loss: 8.454324722290039\n",
      "epoch: 158, loss: 8.388213157653809\n",
      "epoch: 159, loss: 8.322718620300293\n",
      "epoch: 160, loss: 8.257835388183594\n",
      "epoch: 161, loss: 8.193557739257812\n",
      "epoch: 162, loss: 8.129878997802734\n",
      "epoch: 163, loss: 8.066794395446777\n",
      "epoch: 164, loss: 8.00429916381836\n",
      "epoch: 165, loss: 7.942385196685791\n",
      "epoch: 166, loss: 7.881049633026123\n",
      "epoch: 167, loss: 7.820285797119141\n",
      "epoch: 168, loss: 7.760090351104736\n",
      "epoch: 169, loss: 7.7004547119140625\n",
      "epoch: 170, loss: 7.641376495361328\n",
      "epoch: 171, loss: 7.58284854888916\n",
      "epoch: 172, loss: 7.524866580963135\n",
      "epoch: 173, loss: 7.4674248695373535\n",
      "epoch: 174, loss: 7.410518646240234\n",
      "epoch: 175, loss: 7.35414457321167\n",
      "epoch: 176, loss: 7.298295497894287\n",
      "epoch: 177, loss: 7.242966651916504\n",
      "epoch: 178, loss: 7.188154220581055\n",
      "epoch: 179, loss: 7.133852958679199\n",
      "epoch: 180, loss: 7.0800580978393555\n",
      "epoch: 181, loss: 7.026764392852783\n",
      "epoch: 182, loss: 6.973967552185059\n",
      "epoch: 183, loss: 6.921663761138916\n",
      "epoch: 184, loss: 6.869846343994141\n",
      "epoch: 185, loss: 6.818512439727783\n",
      "epoch: 186, loss: 6.767657279968262\n",
      "epoch: 187, loss: 6.7172770500183105\n",
      "epoch: 188, loss: 6.667364597320557\n",
      "epoch: 189, loss: 6.617918491363525\n",
      "epoch: 190, loss: 6.568933010101318\n",
      "epoch: 191, loss: 6.520404815673828\n",
      "epoch: 192, loss: 6.472327709197998\n",
      "epoch: 193, loss: 6.424699306488037\n",
      "epoch: 194, loss: 6.377514362335205\n",
      "epoch: 195, loss: 6.330768585205078\n",
      "epoch: 196, loss: 6.284459114074707\n",
      "epoch: 197, loss: 6.238582134246826\n",
      "epoch: 198, loss: 6.193130970001221\n",
      "epoch: 199, loss: 6.148103713989258\n",
      "epoch: 200, loss: 6.103496074676514\n",
      "epoch: 201, loss: 6.059304237365723\n",
      "epoch: 202, loss: 6.015523433685303\n",
      "epoch: 203, loss: 5.972151756286621\n",
      "epoch: 204, loss: 5.929183006286621\n",
      "epoch: 205, loss: 5.886614799499512\n",
      "epoch: 206, loss: 5.844442844390869\n",
      "epoch: 207, loss: 5.802664279937744\n",
      "epoch: 208, loss: 5.761274337768555\n",
      "epoch: 209, loss: 5.720270156860352\n",
      "epoch: 210, loss: 5.6796464920043945\n",
      "epoch: 211, loss: 5.639402866363525\n",
      "epoch: 212, loss: 5.599534034729004\n",
      "epoch: 213, loss: 5.560035705566406\n",
      "epoch: 214, loss: 5.520904064178467\n",
      "epoch: 215, loss: 5.482138156890869\n",
      "epoch: 216, loss: 5.443732738494873\n",
      "epoch: 217, loss: 5.405684471130371\n",
      "epoch: 218, loss: 5.367990493774414\n",
      "epoch: 219, loss: 5.330647945404053\n",
      "epoch: 220, loss: 5.2936530113220215\n",
      "epoch: 221, loss: 5.257001876831055\n",
      "epoch: 222, loss: 5.220691680908203\n",
      "epoch: 223, loss: 5.184720039367676\n",
      "epoch: 224, loss: 5.149082660675049\n",
      "epoch: 225, loss: 5.113777160644531\n",
      "epoch: 226, loss: 5.078800201416016\n",
      "epoch: 227, loss: 5.0441484451293945\n",
      "epoch: 228, loss: 5.009819984436035\n",
      "epoch: 229, loss: 4.9758100509643555\n",
      "epoch: 230, loss: 4.9421162605285645\n",
      "epoch: 231, loss: 4.908736228942871\n",
      "epoch: 232, loss: 4.875667095184326\n",
      "epoch: 233, loss: 4.842904567718506\n",
      "epoch: 234, loss: 4.810447692871094\n",
      "epoch: 235, loss: 4.778292655944824\n",
      "epoch: 236, loss: 4.74643611907959\n",
      "epoch: 237, loss: 4.7148756980896\n",
      "epoch: 238, loss: 4.683609485626221\n",
      "epoch: 239, loss: 4.6526336669921875\n",
      "epoch: 240, loss: 4.621945858001709\n",
      "epoch: 241, loss: 4.591543197631836\n",
      "epoch: 242, loss: 4.561423301696777\n",
      "epoch: 243, loss: 4.531548500061035\n",
      "epoch: 244, loss: 4.496782302856445\n",
      "epoch: 245, loss: 4.673121452331543\n",
      "epoch: 246, loss: 4.489346027374268\n",
      "epoch: 247, loss: 4.460177421569824\n",
      "epoch: 248, loss: 4.431279182434082\n",
      "epoch: 249, loss: 4.402650356292725\n",
      "epoch: 250, loss: 4.374286651611328\n",
      "epoch: 251, loss: 4.34618616104126\n",
      "epoch: 252, loss: 4.318347930908203\n",
      "epoch: 253, loss: 4.290767669677734\n",
      "epoch: 254, loss: 4.263444423675537\n",
      "epoch: 255, loss: 4.2363739013671875\n",
      "epoch: 256, loss: 4.209555149078369\n",
      "epoch: 257, loss: 4.182985782623291\n",
      "epoch: 258, loss: 4.156663417816162\n",
      "epoch: 259, loss: 4.130585193634033\n",
      "epoch: 260, loss: 4.104748725891113\n",
      "epoch: 261, loss: 4.079152584075928\n",
      "epoch: 262, loss: 4.053793907165527\n",
      "epoch: 263, loss: 4.028670787811279\n",
      "epoch: 264, loss: 4.003780841827393\n",
      "epoch: 265, loss: 3.979121446609497\n",
      "epoch: 266, loss: 3.9546916484832764\n",
      "epoch: 267, loss: 3.930488109588623\n",
      "epoch: 268, loss: 3.9065096378326416\n",
      "epoch: 269, loss: 3.8827531337738037\n",
      "epoch: 270, loss: 3.859217405319214\n",
      "epoch: 271, loss: 3.835900068283081\n",
      "epoch: 272, loss: 3.8127989768981934\n",
      "epoch: 273, loss: 3.789912223815918\n",
      "epoch: 274, loss: 3.767237663269043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 275, loss: 3.7447731494903564\n",
      "epoch: 276, loss: 3.7225167751312256\n",
      "epoch: 277, loss: 3.700467586517334\n",
      "epoch: 278, loss: 3.6786224842071533\n",
      "epoch: 279, loss: 3.656980037689209\n",
      "epoch: 280, loss: 3.635538101196289\n",
      "epoch: 281, loss: 3.6142945289611816\n",
      "epoch: 282, loss: 3.5932483673095703\n",
      "epoch: 283, loss: 3.572396993637085\n",
      "epoch: 284, loss: 3.551739454269409\n",
      "epoch: 285, loss: 3.5312724113464355\n",
      "epoch: 286, loss: 3.510995626449585\n",
      "epoch: 287, loss: 3.490906238555908\n",
      "epoch: 288, loss: 3.471003532409668\n",
      "epoch: 289, loss: 3.451284885406494\n",
      "epoch: 290, loss: 3.431749105453491\n",
      "epoch: 291, loss: 3.412393569946289\n",
      "epoch: 292, loss: 3.3932182788848877\n",
      "epoch: 293, loss: 3.3742194175720215\n",
      "epoch: 294, loss: 3.3553969860076904\n",
      "epoch: 295, loss: 3.336749315261841\n",
      "epoch: 296, loss: 3.3182737827301025\n",
      "epoch: 297, loss: 3.299968957901001\n",
      "epoch: 298, loss: 3.2818338871002197\n",
      "epoch: 299, loss: 3.263866662979126\n",
      "epoch: 300, loss: 3.246065378189087\n",
      "epoch: 301, loss: 3.228429079055786\n",
      "epoch: 302, loss: 3.2109556198120117\n",
      "epoch: 303, loss: 3.1936440467834473\n",
      "epoch: 304, loss: 3.176492691040039\n",
      "epoch: 305, loss: 3.1594998836517334\n",
      "epoch: 306, loss: 3.1426644325256348\n",
      "epoch: 307, loss: 3.125983953475952\n",
      "epoch: 308, loss: 3.1094582080841064\n",
      "epoch: 309, loss: 3.0930850505828857\n",
      "epoch: 310, loss: 3.0768635272979736\n",
      "epoch: 311, loss: 3.060791254043579\n",
      "epoch: 312, loss: 3.044867992401123\n",
      "epoch: 313, loss: 3.0290920734405518\n",
      "epoch: 314, loss: 3.0134613513946533\n",
      "epoch: 315, loss: 2.9979751110076904\n",
      "epoch: 316, loss: 2.9826321601867676\n",
      "epoch: 317, loss: 2.967430591583252\n",
      "epoch: 318, loss: 2.9523696899414062\n",
      "epoch: 319, loss: 2.9374477863311768\n",
      "epoch: 320, loss: 2.9226632118225098\n",
      "epoch: 321, loss: 2.908015251159668\n",
      "epoch: 322, loss: 2.893502950668335\n",
      "epoch: 323, loss: 2.879124164581299\n",
      "epoch: 324, loss: 2.8648784160614014\n",
      "epoch: 325, loss: 2.8507637977600098\n",
      "epoch: 326, loss: 2.8367793560028076\n",
      "epoch: 327, loss: 2.822923421859741\n",
      "epoch: 328, loss: 2.8091957569122314\n",
      "epoch: 329, loss: 2.7955946922302246\n",
      "epoch: 330, loss: 2.782119035720825\n",
      "epoch: 331, loss: 2.7687675952911377\n",
      "epoch: 332, loss: 2.7555389404296875\n",
      "epoch: 333, loss: 2.7424325942993164\n",
      "epoch: 334, loss: 2.7294466495513916\n",
      "epoch: 335, loss: 2.716580629348755\n",
      "epoch: 336, loss: 2.7038328647613525\n",
      "epoch: 337, loss: 2.6912026405334473\n",
      "epoch: 338, loss: 2.6786887645721436\n",
      "epoch: 339, loss: 2.666290283203125\n",
      "epoch: 340, loss: 2.654005289077759\n",
      "epoch: 341, loss: 2.641833782196045\n",
      "epoch: 342, loss: 2.6297740936279297\n",
      "epoch: 343, loss: 2.617825746536255\n",
      "epoch: 344, loss: 2.605987071990967\n",
      "epoch: 345, loss: 2.594257354736328\n",
      "epoch: 346, loss: 2.5826354026794434\n",
      "epoch: 347, loss: 2.571120500564575\n",
      "epoch: 348, loss: 2.559711217880249\n",
      "epoch: 349, loss: 2.5484070777893066\n",
      "epoch: 350, loss: 2.5372066497802734\n",
      "epoch: 351, loss: 2.526109218597412\n",
      "epoch: 352, loss: 2.515113592147827\n",
      "epoch: 353, loss: 2.5042190551757812\n",
      "epoch: 354, loss: 2.493424415588379\n",
      "epoch: 355, loss: 2.482729196548462\n",
      "epoch: 356, loss: 2.4721322059631348\n",
      "epoch: 357, loss: 2.461632251739502\n",
      "epoch: 358, loss: 2.4512290954589844\n",
      "epoch: 359, loss: 2.4409210681915283\n",
      "epoch: 360, loss: 2.4307076930999756\n",
      "epoch: 361, loss: 2.4205877780914307\n",
      "epoch: 362, loss: 2.4105608463287354\n",
      "epoch: 363, loss: 2.400625705718994\n",
      "epoch: 364, loss: 2.3907816410064697\n",
      "epoch: 365, loss: 2.381028413772583\n",
      "epoch: 366, loss: 2.371363639831543\n",
      "epoch: 367, loss: 2.361788272857666\n",
      "epoch: 368, loss: 2.3523001670837402\n",
      "epoch: 369, loss: 2.3428990840911865\n",
      "epoch: 370, loss: 2.3335840702056885\n",
      "epoch: 371, loss: 2.3243541717529297\n",
      "epoch: 372, loss: 2.315208911895752\n",
      "epoch: 373, loss: 2.306147336959839\n",
      "epoch: 374, loss: 2.297168731689453\n",
      "epoch: 375, loss: 2.2882721424102783\n",
      "epoch: 376, loss: 2.279456853866577\n",
      "epoch: 377, loss: 2.2707226276397705\n",
      "epoch: 378, loss: 2.2620677947998047\n",
      "epoch: 379, loss: 2.2534918785095215\n",
      "epoch: 380, loss: 2.244994878768921\n",
      "epoch: 381, loss: 2.23657488822937\n",
      "epoch: 382, loss: 2.2282323837280273\n",
      "epoch: 383, loss: 2.219965696334839\n",
      "epoch: 384, loss: 2.2117748260498047\n",
      "epoch: 385, loss: 2.2036585807800293\n",
      "epoch: 386, loss: 2.1956162452697754\n",
      "epoch: 387, loss: 2.187647581100464\n",
      "epoch: 388, loss: 2.1797516345977783\n",
      "epoch: 389, loss: 2.1719272136688232\n",
      "epoch: 390, loss: 2.1641743183135986\n",
      "epoch: 391, loss: 2.156491994857788\n",
      "epoch: 392, loss: 2.1488797664642334\n",
      "epoch: 393, loss: 2.1413371562957764\n",
      "epoch: 394, loss: 2.1338632106781006\n",
      "epoch: 395, loss: 2.1264569759368896\n",
      "epoch: 396, loss: 2.1191182136535645\n",
      "epoch: 397, loss: 2.111846446990967\n",
      "epoch: 398, loss: 2.1046407222747803\n",
      "epoch: 399, loss: 2.0975005626678467\n",
      "epoch: 400, loss: 2.0904252529144287\n",
      "epoch: 401, loss: 2.0834145545959473\n",
      "epoch: 402, loss: 2.076467275619507\n",
      "epoch: 403, loss: 2.069582939147949\n",
      "epoch: 404, loss: 2.0627615451812744\n",
      "epoch: 405, loss: 2.056001901626587\n",
      "epoch: 406, loss: 2.0493040084838867\n",
      "epoch: 407, loss: 2.042666435241699\n",
      "epoch: 408, loss: 2.0360894203186035\n",
      "epoch: 409, loss: 2.029572010040283\n",
      "epoch: 410, loss: 2.023113489151001\n",
      "epoch: 411, loss: 2.016713857650757\n",
      "epoch: 412, loss: 2.0103719234466553\n",
      "epoch: 413, loss: 2.004087448120117\n",
      "epoch: 414, loss: 1.9978601932525635\n",
      "epoch: 415, loss: 1.9916889667510986\n",
      "epoch: 416, loss: 1.9855738878250122\n",
      "epoch: 417, loss: 1.9795140027999878\n",
      "epoch: 418, loss: 1.9735089540481567\n",
      "epoch: 419, loss: 1.9675582647323608\n",
      "epoch: 420, loss: 1.961661458015442\n",
      "epoch: 421, loss: 1.9558178186416626\n",
      "epoch: 422, loss: 1.9500269889831543\n",
      "epoch: 423, loss: 1.9442884922027588\n",
      "epoch: 424, loss: 1.9386019706726074\n",
      "epoch: 425, loss: 1.9329664707183838\n",
      "epoch: 426, loss: 1.9273818731307983\n",
      "epoch: 427, loss: 1.9218480587005615\n",
      "epoch: 428, loss: 1.9163635969161987\n",
      "epoch: 429, loss: 1.9109288454055786\n",
      "epoch: 430, loss: 1.9055430889129639\n",
      "epoch: 431, loss: 1.9002058506011963\n",
      "epoch: 432, loss: 1.8949164152145386\n",
      "epoch: 433, loss: 1.8896750211715698\n",
      "epoch: 434, loss: 1.8844807147979736\n",
      "epoch: 435, loss: 1.8793329000473022\n",
      "epoch: 436, loss: 1.8742315769195557\n",
      "epoch: 437, loss: 1.8691761493682861\n",
      "epoch: 438, loss: 1.8641659021377563\n",
      "epoch: 439, loss: 1.8592008352279663\n",
      "epoch: 440, loss: 1.8542805910110474\n",
      "epoch: 441, loss: 1.849404215812683\n",
      "epoch: 442, loss: 1.8445714712142944\n",
      "epoch: 443, loss: 1.8397823572158813\n",
      "epoch: 444, loss: 1.8350359201431274\n",
      "epoch: 445, loss: 1.8303321599960327\n",
      "epoch: 446, loss: 1.8256703615188599\n",
      "epoch: 447, loss: 1.821050763130188\n",
      "epoch: 448, loss: 1.816472053527832\n",
      "epoch: 449, loss: 1.811934471130371\n",
      "epoch: 450, loss: 1.8074376583099365\n",
      "epoch: 451, loss: 1.8029807806015015\n",
      "epoch: 452, loss: 1.798563838005066\n",
      "epoch: 453, loss: 1.7941864728927612\n",
      "epoch: 454, loss: 1.7898480892181396\n",
      "epoch: 455, loss: 1.785548448562622\n",
      "epoch: 456, loss: 1.7812871932983398\n",
      "epoch: 457, loss: 1.7770639657974243\n",
      "epoch: 458, loss: 1.7728781700134277\n",
      "epoch: 459, loss: 1.7687300443649292\n",
      "epoch: 460, loss: 1.7646185159683228\n",
      "epoch: 461, loss: 1.7605435848236084\n",
      "epoch: 462, loss: 1.7565053701400757\n",
      "epoch: 463, loss: 1.7525025606155396\n",
      "epoch: 464, loss: 1.748535394668579\n",
      "epoch: 465, loss: 1.7446036338806152\n",
      "epoch: 466, loss: 1.7407068014144897\n",
      "epoch: 467, loss: 1.736844539642334\n",
      "epoch: 468, loss: 1.7330164909362793\n",
      "epoch: 469, loss: 1.7292225360870361\n",
      "epoch: 470, loss: 1.7254619598388672\n",
      "epoch: 471, loss: 1.721734881401062\n",
      "epoch: 472, loss: 1.7180408239364624\n",
      "epoch: 473, loss: 1.7143791913986206\n",
      "epoch: 474, loss: 1.7107503414154053\n",
      "epoch: 475, loss: 1.7071534395217896\n",
      "epoch: 476, loss: 1.7035880088806152\n",
      "epoch: 477, loss: 1.7000545263290405\n",
      "epoch: 478, loss: 1.6965522766113281\n",
      "epoch: 479, loss: 1.6930804252624512\n",
      "epoch: 480, loss: 1.6896395683288574\n",
      "epoch: 481, loss: 1.6862289905548096\n",
      "epoch: 482, loss: 1.6828484535217285\n",
      "epoch: 483, loss: 1.6794977188110352\n",
      "epoch: 484, loss: 1.6761764287948608\n",
      "epoch: 485, loss: 1.6728843450546265\n",
      "epoch: 486, loss: 1.669621229171753\n",
      "epoch: 487, loss: 1.6663867235183716\n",
      "epoch: 488, loss: 1.6631804704666138\n",
      "epoch: 489, loss: 1.660002589225769\n",
      "epoch: 490, loss: 1.6568526029586792\n",
      "epoch: 491, loss: 1.653730034828186\n",
      "epoch: 492, loss: 1.650635004043579\n",
      "epoch: 493, loss: 1.6475669145584106\n",
      "epoch: 494, loss: 1.6445257663726807\n",
      "epoch: 495, loss: 1.641511082649231\n",
      "epoch: 496, loss: 1.6385228633880615\n",
      "epoch: 497, loss: 1.6355606317520142\n",
      "epoch: 498, loss: 1.6326243877410889\n",
      "epoch: 499, loss: 1.6297138929367065\n",
      "epoch: 500, loss: 1.6268285512924194\n",
      "epoch: 501, loss: 1.623968243598938\n",
      "epoch: 502, loss: 1.6211329698562622\n",
      "epoch: 503, loss: 1.6183223724365234\n",
      "epoch: 504, loss: 1.6155362129211426\n",
      "epoch: 505, loss: 1.6127742528915405\n",
      "epoch: 506, loss: 1.6100361347198486\n",
      "epoch: 507, loss: 1.607321858406067\n",
      "epoch: 508, loss: 1.6046313047409058\n",
      "epoch: 509, loss: 1.601963996887207\n",
      "epoch: 510, loss: 1.5993196964263916\n",
      "epoch: 511, loss: 1.59669828414917\n",
      "epoch: 512, loss: 1.5940996408462524\n",
      "epoch: 513, loss: 1.5915234088897705\n",
      "epoch: 514, loss: 1.588969349861145\n",
      "epoch: 515, loss: 1.5864373445510864\n",
      "epoch: 516, loss: 1.5839272737503052\n",
      "epoch: 517, loss: 1.581438660621643\n",
      "epoch: 518, loss: 1.5789715051651\n",
      "epoch: 519, loss: 1.5765255689620972\n",
      "epoch: 520, loss: 1.5741008520126343\n",
      "epoch: 521, loss: 1.5716968774795532\n",
      "epoch: 522, loss: 1.569313406944275\n",
      "epoch: 523, loss: 1.5669504404067993\n",
      "epoch: 524, loss: 1.5646076202392578\n",
      "epoch: 525, loss: 1.5622851848602295\n",
      "epoch: 526, loss: 1.5599826574325562\n",
      "epoch: 527, loss: 1.5576995611190796\n",
      "epoch: 528, loss: 1.5554360151290894\n",
      "epoch: 529, loss: 1.5531917810440063\n",
      "epoch: 530, loss: 1.550966739654541\n",
      "epoch: 531, loss: 1.5487605333328247\n",
      "epoch: 532, loss: 1.546573281288147\n",
      "epoch: 533, loss: 1.54440438747406\n",
      "epoch: 534, loss: 1.5422542095184326\n",
      "epoch: 535, loss: 1.540122151374817\n",
      "epoch: 536, loss: 1.5380080938339233\n",
      "epoch: 537, loss: 1.5359123945236206\n",
      "epoch: 538, loss: 1.5338339805603027\n",
      "epoch: 539, loss: 1.5317734479904175\n",
      "epoch: 540, loss: 1.5297300815582275\n",
      "epoch: 541, loss: 1.527704119682312\n",
      "epoch: 542, loss: 1.5256954431533813\n",
      "epoch: 543, loss: 1.5237032175064087\n",
      "epoch: 544, loss: 1.521728277206421\n",
      "epoch: 545, loss: 1.5197696685791016\n",
      "epoch: 546, loss: 1.5178277492523193\n",
      "epoch: 547, loss: 1.515902042388916\n",
      "epoch: 548, loss: 1.5139925479888916\n",
      "epoch: 549, loss: 1.512099027633667\n",
      "epoch: 550, loss: 1.5102213621139526\n",
      "epoch: 551, loss: 1.508359432220459\n",
      "epoch: 552, loss: 1.5065131187438965\n",
      "epoch: 553, loss: 1.5046820640563965\n",
      "epoch: 554, loss: 1.5028550624847412\n",
      "epoch: 555, loss: 1.4977022409439087\n",
      "epoch: 556, loss: 1.5081262588500977\n",
      "epoch: 557, loss: 1.499472975730896\n",
      "epoch: 558, loss: 1.4976998567581177\n",
      "epoch: 559, loss: 1.4959415197372437\n",
      "epoch: 560, loss: 1.4941977262496948\n",
      "epoch: 561, loss: 1.4924683570861816\n",
      "epoch: 562, loss: 1.4907535314559937\n",
      "epoch: 563, loss: 1.489052653312683\n",
      "epoch: 564, loss: 1.4873660802841187\n",
      "epoch: 565, loss: 1.4856934547424316\n",
      "epoch: 566, loss: 1.484034538269043\n",
      "epoch: 567, loss: 1.4823890924453735\n",
      "epoch: 568, loss: 1.480757474899292\n",
      "epoch: 569, loss: 1.4791392087936401\n",
      "epoch: 570, loss: 1.477534294128418\n",
      "epoch: 571, loss: 1.4759423732757568\n",
      "epoch: 572, loss: 1.4743636846542358\n",
      "epoch: 573, loss: 1.4727977514266968\n",
      "epoch: 574, loss: 1.4712446928024292\n",
      "epoch: 575, loss: 1.4697043895721436\n",
      "epoch: 576, loss: 1.4681766033172607\n",
      "epoch: 577, loss: 1.4666613340377808\n",
      "epoch: 578, loss: 1.465158224105835\n",
      "epoch: 579, loss: 1.463667392730713\n",
      "epoch: 580, loss: 1.462188720703125\n",
      "epoch: 581, loss: 1.4607219696044922\n",
      "epoch: 582, loss: 1.4592673778533936\n",
      "epoch: 583, loss: 1.4578242301940918\n",
      "epoch: 584, loss: 1.4563928842544556\n",
      "epoch: 585, loss: 1.4549732208251953\n",
      "epoch: 586, loss: 1.4535647630691528\n",
      "epoch: 587, loss: 1.4521675109863281\n",
      "epoch: 588, loss: 1.4507818222045898\n",
      "epoch: 589, loss: 1.4494068622589111\n",
      "epoch: 590, loss: 1.4480435848236084\n",
      "epoch: 591, loss: 1.4466906785964966\n",
      "epoch: 592, loss: 1.445348858833313\n",
      "epoch: 593, loss: 1.4440175294876099\n",
      "epoch: 594, loss: 1.4426968097686768\n",
      "epoch: 595, loss: 1.4413866996765137\n",
      "epoch: 596, loss: 1.440087080001831\n",
      "epoch: 597, loss: 1.4387975931167603\n",
      "epoch: 598, loss: 1.43751859664917\n",
      "epoch: 599, loss: 1.4362496137619019\n",
      "epoch: 600, loss: 1.4349905252456665\n",
      "epoch: 601, loss: 1.433741569519043\n",
      "epoch: 602, loss: 1.432502269744873\n",
      "epoch: 603, loss: 1.4312344789505005\n",
      "epoch: 604, loss: 1.4250357151031494\n",
      "epoch: 605, loss: 1.5041067600250244\n",
      "epoch: 606, loss: 1.4412490129470825\n",
      "epoch: 607, loss: 1.4399333000183105\n",
      "epoch: 608, loss: 1.4386284351348877\n",
      "epoch: 609, loss: 1.4373337030410767\n",
      "epoch: 610, loss: 1.4360496997833252\n",
      "epoch: 611, loss: 1.434775471687317\n",
      "epoch: 612, loss: 1.4335113763809204\n",
      "epoch: 613, loss: 1.4322571754455566\n",
      "epoch: 614, loss: 1.4310131072998047\n",
      "epoch: 615, loss: 1.4297786951065063\n",
      "epoch: 616, loss: 1.4285541772842407\n",
      "epoch: 617, loss: 1.4273390769958496\n",
      "epoch: 618, loss: 1.4261337518692017\n",
      "epoch: 619, loss: 1.4249376058578491\n",
      "epoch: 620, loss: 1.423750877380371\n",
      "epoch: 621, loss: 1.4225733280181885\n",
      "epoch: 622, loss: 1.4214050769805908\n",
      "epoch: 623, loss: 1.4202460050582886\n",
      "epoch: 624, loss: 1.4190959930419922\n",
      "epoch: 625, loss: 1.417954683303833\n",
      "epoch: 626, loss: 1.4168223142623901\n",
      "epoch: 627, loss: 1.4156988859176636\n",
      "epoch: 628, loss: 1.4145839214324951\n",
      "epoch: 629, loss: 1.4134775400161743\n",
      "epoch: 630, loss: 1.4123799800872803\n",
      "epoch: 631, loss: 1.4112904071807861\n",
      "epoch: 632, loss: 1.4102095365524292\n",
      "epoch: 633, loss: 1.4091366529464722\n",
      "epoch: 634, loss: 1.4080723524093628\n",
      "epoch: 635, loss: 1.4070160388946533\n",
      "epoch: 636, loss: 1.4059675931930542\n",
      "epoch: 637, loss: 1.4049272537231445\n",
      "epoch: 638, loss: 1.4038949012756348\n",
      "epoch: 639, loss: 1.4028701782226562\n",
      "epoch: 640, loss: 1.4018534421920776\n",
      "epoch: 641, loss: 1.4008443355560303\n",
      "epoch: 642, loss: 1.399842619895935\n",
      "epoch: 643, loss: 1.398848533630371\n",
      "epoch: 644, loss: 1.3978620767593384\n",
      "epoch: 645, loss: 1.3968830108642578\n",
      "epoch: 646, loss: 1.3959110975265503\n",
      "epoch: 647, loss: 1.3949466943740845\n",
      "epoch: 648, loss: 1.3939894437789917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 649, loss: 1.3930391073226929\n",
      "epoch: 650, loss: 1.392095923423767\n",
      "epoch: 651, loss: 1.3911596536636353\n",
      "epoch: 652, loss: 1.390230655670166\n",
      "epoch: 653, loss: 1.3893083333969116\n",
      "epoch: 654, loss: 1.3883928060531616\n",
      "epoch: 655, loss: 1.387483835220337\n",
      "epoch: 656, loss: 1.386581540107727\n",
      "epoch: 657, loss: 1.3856861591339111\n",
      "epoch: 658, loss: 1.3847970962524414\n",
      "epoch: 659, loss: 1.3839147090911865\n",
      "epoch: 660, loss: 1.383038878440857\n",
      "epoch: 661, loss: 1.3821691274642944\n",
      "epoch: 662, loss: 1.3813056945800781\n",
      "epoch: 663, loss: 1.3804484605789185\n",
      "epoch: 664, loss: 1.379597544670105\n",
      "epoch: 665, loss: 1.3787529468536377\n",
      "epoch: 666, loss: 1.3778150081634521\n",
      "epoch: 667, loss: 1.37073814868927\n",
      "epoch: 668, loss: 1.6953089237213135\n",
      "epoch: 669, loss: 1.4184107780456543\n",
      "epoch: 670, loss: 1.4172008037567139\n",
      "epoch: 671, loss: 1.4160003662109375\n",
      "epoch: 672, loss: 1.414809226989746\n",
      "epoch: 673, loss: 1.4136276245117188\n",
      "epoch: 674, loss: 1.4124552011489868\n",
      "epoch: 675, loss: 1.4112919569015503\n",
      "epoch: 676, loss: 1.4101375341415405\n",
      "epoch: 677, loss: 1.4089921712875366\n",
      "epoch: 678, loss: 1.407855749130249\n",
      "epoch: 679, loss: 1.4067282676696777\n",
      "epoch: 680, loss: 1.4056092500686646\n",
      "epoch: 681, loss: 1.4044990539550781\n",
      "epoch: 682, loss: 1.4033973217010498\n",
      "epoch: 683, loss: 1.4023041725158691\n",
      "epoch: 684, loss: 1.401219367980957\n",
      "epoch: 685, loss: 1.4001432657241821\n",
      "epoch: 686, loss: 1.3990750312805176\n",
      "epoch: 687, loss: 1.398015022277832\n",
      "epoch: 688, loss: 1.396963119506836\n",
      "epoch: 689, loss: 1.3959192037582397\n",
      "epoch: 690, loss: 1.3948832750320435\n",
      "epoch: 691, loss: 1.3938552141189575\n",
      "epoch: 692, loss: 1.3928351402282715\n",
      "epoch: 693, loss: 1.3918224573135376\n",
      "epoch: 694, loss: 1.390817642211914\n",
      "epoch: 695, loss: 1.3898203372955322\n",
      "epoch: 696, loss: 1.3888307809829712\n",
      "epoch: 697, loss: 1.3878483772277832\n",
      "epoch: 698, loss: 1.3868733644485474\n",
      "epoch: 699, loss: 1.3859058618545532\n",
      "epoch: 700, loss: 1.3849455118179321\n",
      "epoch: 701, loss: 1.383992314338684\n",
      "epoch: 702, loss: 1.3830461502075195\n",
      "epoch: 703, loss: 1.382107138633728\n",
      "epoch: 704, loss: 1.38117516040802\n",
      "epoch: 705, loss: 1.3802496194839478\n",
      "epoch: 706, loss: 1.3793314695358276\n",
      "epoch: 707, loss: 1.3784196376800537\n",
      "epoch: 708, loss: 1.377515196800232\n",
      "epoch: 709, loss: 1.3766170740127563\n",
      "epoch: 710, loss: 1.3757253885269165\n",
      "epoch: 711, loss: 1.3748401403427124\n",
      "epoch: 712, loss: 1.3739615678787231\n",
      "epoch: 713, loss: 1.37308931350708\n",
      "epoch: 714, loss: 1.3722236156463623\n",
      "epoch: 715, loss: 1.3713642358779907\n",
      "epoch: 716, loss: 1.3705106973648071\n",
      "epoch: 717, loss: 1.3696634769439697\n",
      "epoch: 718, loss: 1.368822693824768\n",
      "epoch: 719, loss: 1.3679875135421753\n",
      "epoch: 720, loss: 1.3671585321426392\n",
      "epoch: 721, loss: 1.3663355112075806\n",
      "epoch: 722, loss: 1.3655184507369995\n",
      "epoch: 723, loss: 1.3647072315216064\n",
      "epoch: 724, loss: 1.3639016151428223\n",
      "epoch: 725, loss: 1.3631017208099365\n",
      "epoch: 726, loss: 1.3623077869415283\n",
      "epoch: 727, loss: 1.3615190982818604\n",
      "epoch: 728, loss: 1.3607362508773804\n",
      "epoch: 729, loss: 1.3599588871002197\n",
      "epoch: 730, loss: 1.3591870069503784\n",
      "epoch: 731, loss: 1.358420491218567\n",
      "epoch: 732, loss: 1.3576593399047852\n",
      "epoch: 733, loss: 1.3569036722183228\n",
      "epoch: 734, loss: 1.3561530113220215\n",
      "epoch: 735, loss: 1.3554075956344604\n",
      "epoch: 736, loss: 1.3546675443649292\n",
      "epoch: 737, loss: 1.3539326190948486\n",
      "epoch: 738, loss: 1.35320246219635\n",
      "epoch: 739, loss: 1.3524774312973022\n",
      "epoch: 740, loss: 1.3517574071884155\n",
      "epoch: 741, loss: 1.35104238986969\n",
      "epoch: 742, loss: 1.350332260131836\n",
      "epoch: 743, loss: 1.349627137184143\n",
      "epoch: 744, loss: 1.3489266633987427\n",
      "epoch: 745, loss: 1.3482306003570557\n",
      "epoch: 746, loss: 1.3475396633148193\n",
      "epoch: 747, loss: 1.346853256225586\n",
      "epoch: 748, loss: 1.3461713790893555\n",
      "epoch: 749, loss: 1.3454941511154175\n",
      "epoch: 750, loss: 1.3448214530944824\n",
      "epoch: 751, loss: 1.3441535234451294\n",
      "epoch: 752, loss: 1.343489646911621\n",
      "epoch: 753, loss: 1.3428304195404053\n",
      "epoch: 754, loss: 1.3421753644943237\n",
      "epoch: 755, loss: 1.3415247201919556\n",
      "epoch: 756, loss: 1.3408786058425903\n",
      "epoch: 757, loss: 1.3402364253997803\n",
      "epoch: 758, loss: 1.339598536491394\n",
      "epoch: 759, loss: 1.3389647006988525\n",
      "epoch: 760, loss: 1.3383351564407349\n",
      "epoch: 761, loss: 1.337709665298462\n",
      "epoch: 762, loss: 1.3370883464813232\n",
      "epoch: 763, loss: 1.3364708423614502\n",
      "epoch: 764, loss: 1.3358575105667114\n",
      "epoch: 765, loss: 1.3352479934692383\n",
      "epoch: 766, loss: 1.3346424102783203\n",
      "epoch: 767, loss: 1.3340405225753784\n",
      "epoch: 768, loss: 1.3334428071975708\n",
      "epoch: 769, loss: 1.3328486680984497\n",
      "epoch: 770, loss: 1.3322583436965942\n",
      "epoch: 771, loss: 1.3316717147827148\n",
      "epoch: 772, loss: 1.3310887813568115\n",
      "epoch: 773, loss: 1.3305096626281738\n",
      "epoch: 774, loss: 1.3299338817596436\n",
      "epoch: 775, loss: 1.3293617963790894\n",
      "epoch: 776, loss: 1.3287934064865112\n",
      "epoch: 777, loss: 1.3282283544540405\n",
      "epoch: 778, loss: 1.3276668787002563\n",
      "epoch: 779, loss: 1.3271088600158691\n",
      "epoch: 780, loss: 1.3265544176101685\n",
      "epoch: 781, loss: 1.3260031938552856\n",
      "epoch: 782, loss: 1.3254553079605103\n",
      "epoch: 783, loss: 1.3249108791351318\n",
      "epoch: 784, loss: 1.3243697881698608\n",
      "epoch: 785, loss: 1.3238316774368286\n",
      "epoch: 786, loss: 1.3232972621917725\n",
      "epoch: 787, loss: 1.322765588760376\n",
      "epoch: 788, loss: 1.322237253189087\n",
      "epoch: 789, loss: 1.3217122554779053\n",
      "epoch: 790, loss: 1.3211901187896729\n",
      "epoch: 791, loss: 1.3206712007522583\n",
      "epoch: 792, loss: 1.320155382156372\n",
      "epoch: 793, loss: 1.3196425437927246\n",
      "epoch: 794, loss: 1.319132924079895\n",
      "epoch: 795, loss: 1.318626046180725\n",
      "epoch: 796, loss: 1.3181220293045044\n",
      "epoch: 797, loss: 1.317621111869812\n",
      "epoch: 798, loss: 1.3171229362487793\n",
      "epoch: 799, loss: 1.316628098487854\n",
      "epoch: 800, loss: 1.3161355257034302\n",
      "epoch: 801, loss: 1.3156460523605347\n",
      "epoch: 802, loss: 1.3151593208312988\n",
      "epoch: 803, loss: 1.3146754503250122\n",
      "epoch: 804, loss: 1.3141939640045166\n",
      "epoch: 805, loss: 1.3137155771255493\n",
      "epoch: 806, loss: 1.3132398128509521\n",
      "epoch: 807, loss: 1.312766671180725\n",
      "epoch: 808, loss: 1.3122960329055786\n",
      "epoch: 809, loss: 1.3118282556533813\n",
      "epoch: 810, loss: 1.3113629817962646\n",
      "epoch: 811, loss: 1.3109002113342285\n",
      "epoch: 812, loss: 1.3104400634765625\n",
      "epoch: 813, loss: 1.3099822998046875\n",
      "epoch: 814, loss: 1.309527039527893\n",
      "epoch: 815, loss: 1.3090742826461792\n",
      "epoch: 816, loss: 1.3086241483688354\n",
      "epoch: 817, loss: 1.3081762790679932\n",
      "epoch: 818, loss: 1.307730793952942\n",
      "epoch: 819, loss: 1.3072879314422607\n",
      "epoch: 820, loss: 1.306847095489502\n",
      "epoch: 821, loss: 1.3064088821411133\n",
      "epoch: 822, loss: 1.305972933769226\n",
      "epoch: 823, loss: 1.3055390119552612\n",
      "epoch: 824, loss: 1.3051077127456665\n",
      "epoch: 825, loss: 1.3046784400939941\n",
      "epoch: 826, loss: 1.3042515516281128\n",
      "epoch: 827, loss: 1.303826928138733\n",
      "epoch: 828, loss: 1.3034043312072754\n",
      "epoch: 829, loss: 1.3029838800430298\n",
      "epoch: 830, loss: 1.3025656938552856\n",
      "epoch: 831, loss: 1.3021496534347534\n",
      "epoch: 832, loss: 1.3017356395721436\n",
      "epoch: 833, loss: 1.3013237714767456\n",
      "epoch: 834, loss: 1.30091392993927\n",
      "epoch: 835, loss: 1.3005062341690063\n",
      "epoch: 836, loss: 1.3001004457473755\n",
      "epoch: 837, loss: 1.2996968030929565\n",
      "epoch: 838, loss: 1.29929518699646\n",
      "epoch: 839, loss: 1.2988953590393066\n",
      "epoch: 840, loss: 1.2984098196029663\n",
      "epoch: 841, loss: 1.2924052476882935\n",
      "epoch: 842, loss: 1.297977328300476\n",
      "epoch: 843, loss: 1.297582983970642\n",
      "epoch: 844, loss: 1.2971906661987305\n",
      "epoch: 845, loss: 1.2968002557754517\n",
      "epoch: 846, loss: 1.2964115142822266\n",
      "epoch: 847, loss: 1.2960247993469238\n",
      "epoch: 848, loss: 1.2956398725509644\n",
      "epoch: 849, loss: 1.2952567338943481\n",
      "epoch: 850, loss: 1.2948755025863647\n",
      "epoch: 851, loss: 1.294495940208435\n",
      "epoch: 852, loss: 1.2941182851791382\n",
      "epoch: 853, loss: 1.293742299079895\n",
      "epoch: 854, loss: 1.2933681011199951\n",
      "epoch: 855, loss: 1.2929956912994385\n",
      "epoch: 856, loss: 1.2926249504089355\n",
      "epoch: 857, loss: 1.2921086549758911\n",
      "epoch: 858, loss: 1.2850978374481201\n",
      "epoch: 859, loss: 1.2918694019317627\n",
      "epoch: 860, loss: 1.2915023565292358\n",
      "epoch: 861, loss: 1.2911370992660522\n",
      "epoch: 862, loss: 1.2907732725143433\n",
      "epoch: 863, loss: 1.2904112339019775\n",
      "epoch: 864, loss: 1.2900506258010864\n",
      "epoch: 865, loss: 1.2896918058395386\n",
      "epoch: 866, loss: 1.2893344163894653\n",
      "epoch: 867, loss: 1.2889788150787354\n",
      "epoch: 868, loss: 1.2886242866516113\n",
      "epoch: 869, loss: 1.2882717847824097\n",
      "epoch: 870, loss: 1.2879207134246826\n",
      "epoch: 871, loss: 1.2875711917877197\n",
      "epoch: 872, loss: 1.2872228622436523\n",
      "epoch: 873, loss: 1.2868763208389282\n",
      "epoch: 874, loss: 1.2865313291549683\n",
      "epoch: 875, loss: 1.2861876487731934\n",
      "epoch: 876, loss: 1.2858455181121826\n",
      "epoch: 877, loss: 1.2855044603347778\n",
      "epoch: 878, loss: 1.2849375009536743\n",
      "epoch: 879, loss: 1.2769614458084106\n",
      "epoch: 880, loss: 1.2850539684295654\n",
      "epoch: 881, loss: 1.284713864326477\n",
      "epoch: 882, loss: 1.2843749523162842\n",
      "epoch: 883, loss: 1.284037709236145\n",
      "epoch: 884, loss: 1.28370201587677\n",
      "epoch: 885, loss: 1.2833672761917114\n",
      "epoch: 886, loss: 1.2830342054367065\n",
      "epoch: 887, loss: 1.2827023267745972\n",
      "epoch: 888, loss: 1.2823717594146729\n",
      "epoch: 889, loss: 1.2820426225662231\n",
      "epoch: 890, loss: 1.281714916229248\n",
      "epoch: 891, loss: 1.2813881635665894\n",
      "epoch: 892, loss: 1.2810628414154053\n",
      "epoch: 893, loss: 1.2807389497756958\n",
      "epoch: 894, loss: 1.2804162502288818\n",
      "epoch: 895, loss: 1.2800947427749634\n",
      "epoch: 896, loss: 1.2797746658325195\n",
      "epoch: 897, loss: 1.2794556617736816\n",
      "epoch: 898, loss: 1.2791377305984497\n",
      "epoch: 899, loss: 1.2788212299346924\n",
      "epoch: 900, loss: 1.2785060405731201\n",
      "epoch: 901, loss: 1.2781918048858643\n",
      "epoch: 902, loss: 1.2778788805007935\n",
      "epoch: 903, loss: 1.2775670289993286\n",
      "epoch: 904, loss: 1.2772563695907593\n",
      "epoch: 905, loss: 1.2769469022750854\n",
      "epoch: 906, loss: 1.276638388633728\n",
      "epoch: 907, loss: 1.2763310670852661\n",
      "epoch: 908, loss: 1.2760250568389893\n",
      "epoch: 909, loss: 1.275525689125061\n",
      "epoch: 910, loss: 1.2671693563461304\n",
      "epoch: 911, loss: 1.276090383529663\n",
      "epoch: 912, loss: 1.2757797241210938\n",
      "epoch: 913, loss: 1.2754703760147095\n",
      "epoch: 914, loss: 1.2751623392105103\n",
      "epoch: 915, loss: 1.274855136871338\n",
      "epoch: 916, loss: 1.2745492458343506\n",
      "epoch: 917, loss: 1.2742443084716797\n",
      "epoch: 918, loss: 1.2739405632019043\n",
      "epoch: 919, loss: 1.2736378908157349\n",
      "epoch: 920, loss: 1.2733362913131714\n",
      "epoch: 921, loss: 1.2730356454849243\n",
      "epoch: 922, loss: 1.2727363109588623\n",
      "epoch: 923, loss: 1.2724376916885376\n",
      "epoch: 924, loss: 1.2721402645111084\n",
      "epoch: 925, loss: 1.2718439102172852\n",
      "epoch: 926, loss: 1.2715485095977783\n",
      "epoch: 927, loss: 1.2712541818618774\n",
      "epoch: 928, loss: 1.270960807800293\n",
      "epoch: 929, loss: 1.2706685066223145\n",
      "epoch: 930, loss: 1.2703769207000732\n",
      "epoch: 931, loss: 1.2700865268707275\n",
      "epoch: 932, loss: 1.2697970867156982\n",
      "epoch: 933, loss: 1.2695088386535645\n",
      "epoch: 934, loss: 1.2692211866378784\n",
      "epoch: 935, loss: 1.2689344882965088\n",
      "epoch: 936, loss: 1.2686487436294556\n",
      "epoch: 937, loss: 1.2683640718460083\n",
      "epoch: 938, loss: 1.2680801153182983\n",
      "epoch: 939, loss: 1.2677972316741943\n",
      "epoch: 940, loss: 1.2675151824951172\n",
      "epoch: 941, loss: 1.267233967781067\n",
      "epoch: 942, loss: 1.2669535875320435\n",
      "epoch: 943, loss: 1.2666743993759155\n",
      "epoch: 944, loss: 1.2663956880569458\n",
      "epoch: 945, loss: 1.2661179304122925\n",
      "epoch: 946, loss: 1.2658411264419556\n",
      "epoch: 947, loss: 1.2655651569366455\n",
      "epoch: 948, loss: 1.2652897834777832\n",
      "epoch: 949, loss: 1.2650154829025269\n",
      "epoch: 950, loss: 1.2647420167922974\n",
      "epoch: 951, loss: 1.2644693851470947\n",
      "epoch: 952, loss: 1.2641973495483398\n",
      "epoch: 953, loss: 1.2639262676239014\n",
      "epoch: 954, loss: 1.2636559009552002\n",
      "epoch: 955, loss: 1.2633861303329468\n",
      "epoch: 956, loss: 1.2631174325942993\n",
      "epoch: 957, loss: 1.262737512588501\n",
      "epoch: 958, loss: 1.2543835639953613\n",
      "epoch: 959, loss: 1.2632821798324585\n",
      "epoch: 960, loss: 1.2630078792572021\n",
      "epoch: 961, loss: 1.262734055519104\n",
      "epoch: 962, loss: 1.2624614238739014\n",
      "epoch: 963, loss: 1.262189507484436\n",
      "epoch: 964, loss: 1.261918306350708\n",
      "epoch: 965, loss: 1.2616479396820068\n",
      "epoch: 966, loss: 1.2613784074783325\n",
      "epoch: 967, loss: 1.261109709739685\n",
      "epoch: 968, loss: 1.2608416080474854\n",
      "epoch: 969, loss: 1.260574460029602\n",
      "epoch: 970, loss: 1.260307788848877\n",
      "epoch: 971, loss: 1.2600420713424683\n",
      "epoch: 972, loss: 1.2597771883010864\n",
      "epoch: 973, loss: 1.2595129013061523\n",
      "epoch: 974, loss: 1.259249210357666\n",
      "epoch: 975, loss: 1.2589863538742065\n",
      "epoch: 976, loss: 1.258724331855774\n",
      "epoch: 977, loss: 1.2584630250930786\n",
      "epoch: 978, loss: 1.258202314376831\n",
      "epoch: 979, loss: 1.2579423189163208\n",
      "epoch: 980, loss: 1.2576829195022583\n",
      "epoch: 981, loss: 1.2574243545532227\n",
      "epoch: 982, loss: 1.2571665048599243\n",
      "epoch: 983, loss: 1.2569091320037842\n",
      "epoch: 984, loss: 1.2566524744033813\n",
      "epoch: 985, loss: 1.2563965320587158\n",
      "epoch: 986, loss: 1.256141185760498\n",
      "epoch: 987, loss: 1.2558865547180176\n",
      "epoch: 988, loss: 1.2556325197219849\n",
      "epoch: 989, loss: 1.255379319190979\n",
      "epoch: 990, loss: 1.2551264762878418\n",
      "epoch: 991, loss: 1.2548744678497314\n",
      "epoch: 992, loss: 1.2546229362487793\n",
      "epoch: 993, loss: 1.2543721199035645\n",
      "epoch: 994, loss: 1.2541217803955078\n",
      "epoch: 995, loss: 1.2538721561431885\n",
      "epoch: 996, loss: 1.253623127937317\n",
      "epoch: 997, loss: 1.253374695777893\n",
      "epoch: 998, loss: 1.2531267404556274\n",
      "epoch: 999, loss: 1.2528795003890991\n"
     ]
    }
   ],
   "source": [
    "shift_model = MLP(61, 65, nn.LeakyReLU)\n",
    "optimizer_shifts = torch.optim.SGD(shift_model.parameters(), lr=1500)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    shifts_pred = shift_model(phonemes)\n",
    "    loss_shift = loss_fn(shifts_pred,shifts)\n",
    "    losses.append(loss_shift)\n",
    "    print(f\"epoch: {epoch}, loss: {loss_shift}\")\n",
    "    optimizer_shifts.zero_grad()\n",
    "    loss_shift.backward()\n",
    "    optimizer_shifts.step()\n",
    "#     if len(losses)>10:\n",
    "#         if losses[-1]>=losses[-10] or losses[-1]>=losses[-5]:\n",
    "#             print(\"detecting minimum validation loss. stopping early\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b853884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 50.91822052001953\n",
      "epoch: 1, loss: 34.56864929199219\n",
      "epoch: 2, loss: 33.04054260253906\n",
      "epoch: 3, loss: 32.74462890625\n",
      "epoch: 4, loss: 32.45148849487305\n",
      "epoch: 5, loss: 32.16109085083008\n",
      "epoch: 6, loss: 31.87339973449707\n",
      "epoch: 7, loss: 31.588050842285156\n",
      "epoch: 8, loss: 32.95539855957031\n",
      "epoch: 9, loss: 31.412677764892578\n",
      "epoch: 10, loss: 31.131986618041992\n",
      "epoch: 11, loss: 30.85392189025879\n",
      "epoch: 12, loss: 30.57845115661621\n",
      "epoch: 13, loss: 30.305564880371094\n",
      "epoch: 14, loss: 30.035228729248047\n",
      "epoch: 15, loss: 29.767417907714844\n",
      "epoch: 16, loss: 29.50211524963379\n",
      "epoch: 17, loss: 29.23929214477539\n",
      "epoch: 18, loss: 28.97892951965332\n",
      "epoch: 19, loss: 28.72100067138672\n",
      "epoch: 20, loss: 28.465484619140625\n",
      "epoch: 21, loss: 28.212356567382812\n",
      "epoch: 22, loss: 27.96159553527832\n",
      "epoch: 23, loss: 27.713178634643555\n",
      "epoch: 24, loss: 27.467086791992188\n",
      "epoch: 25, loss: 27.223295211791992\n",
      "epoch: 26, loss: 26.981788635253906\n",
      "epoch: 27, loss: 26.74253273010254\n",
      "epoch: 28, loss: 26.505517959594727\n",
      "epoch: 29, loss: 26.270721435546875\n",
      "epoch: 30, loss: 26.03812026977539\n",
      "epoch: 31, loss: 25.807689666748047\n",
      "epoch: 32, loss: 25.579418182373047\n",
      "epoch: 33, loss: 25.353282928466797\n",
      "epoch: 34, loss: 25.129255294799805\n",
      "epoch: 35, loss: 24.907329559326172\n",
      "epoch: 36, loss: 24.687477111816406\n",
      "epoch: 37, loss: 24.46967887878418\n",
      "epoch: 38, loss: 24.253917694091797\n",
      "epoch: 39, loss: 24.04017448425293\n",
      "epoch: 40, loss: 23.828432083129883\n",
      "epoch: 41, loss: 23.61866569519043\n",
      "epoch: 42, loss: 23.41086196899414\n",
      "epoch: 43, loss: 23.205001831054688\n",
      "epoch: 44, loss: 23.001068115234375\n",
      "epoch: 45, loss: 22.799039840698242\n",
      "epoch: 46, loss: 22.59889793395996\n",
      "epoch: 47, loss: 22.400634765625\n",
      "epoch: 48, loss: 22.20421600341797\n",
      "epoch: 49, loss: 22.009639739990234\n",
      "epoch: 50, loss: 21.81688117980957\n",
      "epoch: 51, loss: 21.62592315673828\n",
      "epoch: 52, loss: 21.436752319335938\n",
      "epoch: 53, loss: 21.249351501464844\n",
      "epoch: 54, loss: 21.063703536987305\n",
      "epoch: 55, loss: 20.87978744506836\n",
      "epoch: 56, loss: 20.69759178161621\n",
      "epoch: 57, loss: 20.517099380493164\n",
      "epoch: 58, loss: 20.33829689025879\n",
      "epoch: 59, loss: 20.161163330078125\n",
      "epoch: 60, loss: 19.985687255859375\n",
      "epoch: 61, loss: 19.81184959411621\n",
      "epoch: 62, loss: 19.639638900756836\n",
      "epoch: 63, loss: 19.469036102294922\n",
      "epoch: 64, loss: 19.300029754638672\n",
      "epoch: 65, loss: 19.132606506347656\n",
      "epoch: 66, loss: 18.96674346923828\n",
      "epoch: 67, loss: 18.802433013916016\n",
      "epoch: 68, loss: 18.639659881591797\n",
      "epoch: 69, loss: 18.478404998779297\n",
      "epoch: 70, loss: 18.31865882873535\n",
      "epoch: 71, loss: 18.1604061126709\n",
      "epoch: 72, loss: 18.003633499145508\n",
      "epoch: 73, loss: 17.848325729370117\n",
      "epoch: 74, loss: 17.69447135925293\n",
      "epoch: 75, loss: 17.542051315307617\n",
      "epoch: 76, loss: 17.391056060791016\n",
      "epoch: 77, loss: 17.241474151611328\n",
      "epoch: 78, loss: 17.093290328979492\n",
      "epoch: 79, loss: 16.94649314880371\n",
      "epoch: 80, loss: 16.801063537597656\n",
      "epoch: 81, loss: 16.656997680664062\n",
      "epoch: 82, loss: 16.51427459716797\n",
      "epoch: 83, loss: 16.372886657714844\n",
      "epoch: 84, loss: 16.23282241821289\n",
      "epoch: 85, loss: 16.094064712524414\n",
      "epoch: 86, loss: 15.95660400390625\n",
      "epoch: 87, loss: 15.820428848266602\n",
      "epoch: 88, loss: 15.685523986816406\n",
      "epoch: 89, loss: 15.551881790161133\n",
      "epoch: 90, loss: 15.419488906860352\n",
      "epoch: 91, loss: 15.288333892822266\n",
      "epoch: 92, loss: 15.158402442932129\n",
      "epoch: 93, loss: 15.029687881469727\n",
      "epoch: 94, loss: 14.902172088623047\n",
      "epoch: 95, loss: 14.77585220336914\n",
      "epoch: 96, loss: 14.650708198547363\n",
      "epoch: 97, loss: 14.526737213134766\n",
      "epoch: 98, loss: 14.403923034667969\n",
      "epoch: 99, loss: 14.282257080078125\n",
      "epoch: 100, loss: 14.161726951599121\n",
      "epoch: 101, loss: 14.042323112487793\n",
      "epoch: 102, loss: 13.92403507232666\n",
      "epoch: 103, loss: 13.806853294372559\n",
      "epoch: 104, loss: 13.690764427185059\n",
      "epoch: 105, loss: 13.575763702392578\n",
      "epoch: 106, loss: 13.461835861206055\n",
      "epoch: 107, loss: 13.348970413208008\n",
      "epoch: 108, loss: 13.237159729003906\n",
      "epoch: 109, loss: 13.126396179199219\n",
      "epoch: 110, loss: 13.016664505004883\n",
      "epoch: 111, loss: 12.907958984375\n",
      "epoch: 112, loss: 12.80026912689209\n",
      "epoch: 113, loss: 12.693584442138672\n",
      "epoch: 114, loss: 12.587898254394531\n",
      "epoch: 115, loss: 12.483199119567871\n",
      "epoch: 116, loss: 12.379475593566895\n",
      "epoch: 117, loss: 12.27672290802002\n",
      "epoch: 118, loss: 12.17492961883545\n",
      "epoch: 119, loss: 12.074088096618652\n",
      "epoch: 120, loss: 11.974185943603516\n",
      "epoch: 121, loss: 11.875218391418457\n",
      "epoch: 122, loss: 11.777175903320312\n",
      "epoch: 123, loss: 11.680048942565918\n",
      "epoch: 124, loss: 11.58382797241211\n",
      "epoch: 125, loss: 11.488507270812988\n",
      "epoch: 126, loss: 11.394075393676758\n",
      "epoch: 127, loss: 11.300525665283203\n",
      "epoch: 128, loss: 11.207849502563477\n",
      "epoch: 129, loss: 11.11603832244873\n",
      "epoch: 130, loss: 11.025087356567383\n",
      "epoch: 131, loss: 10.934983253479004\n",
      "epoch: 132, loss: 10.845720291137695\n",
      "epoch: 133, loss: 10.757292747497559\n",
      "epoch: 134, loss: 10.669689178466797\n",
      "epoch: 135, loss: 10.582904815673828\n",
      "epoch: 136, loss: 10.496930122375488\n",
      "epoch: 137, loss: 10.411757469177246\n",
      "epoch: 138, loss: 10.327381134033203\n",
      "epoch: 139, loss: 10.243791580200195\n",
      "epoch: 140, loss: 10.160983085632324\n",
      "epoch: 141, loss: 10.078947067260742\n",
      "epoch: 142, loss: 9.99767780303955\n",
      "epoch: 143, loss: 9.917166709899902\n",
      "epoch: 144, loss: 9.837407112121582\n",
      "epoch: 145, loss: 9.758392333984375\n",
      "epoch: 146, loss: 9.68011474609375\n",
      "epoch: 147, loss: 9.602569580078125\n",
      "epoch: 148, loss: 9.525747299194336\n",
      "epoch: 149, loss: 9.449642181396484\n",
      "epoch: 150, loss: 9.374247550964355\n",
      "epoch: 151, loss: 9.299555778503418\n",
      "epoch: 152, loss: 9.22556209564209\n",
      "epoch: 153, loss: 9.152257919311523\n",
      "epoch: 154, loss: 9.079638481140137\n",
      "epoch: 155, loss: 9.007697105407715\n",
      "epoch: 156, loss: 8.936427116394043\n",
      "epoch: 157, loss: 8.86582088470459\n",
      "epoch: 158, loss: 8.79587459564209\n",
      "epoch: 159, loss: 8.726582527160645\n",
      "epoch: 160, loss: 8.657934188842773\n",
      "epoch: 161, loss: 8.589927673339844\n",
      "epoch: 162, loss: 8.52255630493164\n",
      "epoch: 163, loss: 8.455814361572266\n",
      "epoch: 164, loss: 8.389693260192871\n",
      "epoch: 165, loss: 8.324190139770508\n",
      "epoch: 166, loss: 8.259297370910645\n",
      "epoch: 167, loss: 8.195011138916016\n",
      "epoch: 168, loss: 8.13132381439209\n",
      "epoch: 169, loss: 8.068231582641602\n",
      "epoch: 170, loss: 8.00572681427002\n",
      "epoch: 171, loss: 7.943805694580078\n",
      "epoch: 172, loss: 7.8824615478515625\n",
      "epoch: 173, loss: 7.821691036224365\n",
      "epoch: 174, loss: 7.761487007141113\n",
      "epoch: 175, loss: 7.70184326171875\n",
      "epoch: 176, loss: 7.642756462097168\n",
      "epoch: 177, loss: 7.584221839904785\n",
      "epoch: 178, loss: 7.526230812072754\n",
      "epoch: 179, loss: 7.468782424926758\n",
      "epoch: 180, loss: 7.411869049072266\n",
      "epoch: 181, loss: 7.355486869812012\n",
      "epoch: 182, loss: 7.299630641937256\n",
      "epoch: 183, loss: 7.2442946434021\n",
      "epoch: 184, loss: 7.189474105834961\n",
      "epoch: 185, loss: 7.135166645050049\n",
      "epoch: 186, loss: 7.081364154815674\n",
      "epoch: 187, loss: 7.02806282043457\n",
      "epoch: 188, loss: 6.975260257720947\n",
      "epoch: 189, loss: 6.922948837280273\n",
      "epoch: 190, loss: 6.871125221252441\n",
      "epoch: 191, loss: 6.819785118103027\n",
      "epoch: 192, loss: 6.768922328948975\n",
      "epoch: 193, loss: 6.71853494644165\n",
      "epoch: 194, loss: 6.66861629486084\n",
      "epoch: 195, loss: 6.619163513183594\n",
      "epoch: 196, loss: 6.57017183303833\n",
      "epoch: 197, loss: 6.52163553237915\n",
      "epoch: 198, loss: 6.47355318069458\n",
      "epoch: 199, loss: 6.425918102264404\n",
      "epoch: 200, loss: 6.378726959228516\n",
      "epoch: 201, loss: 6.331975936889648\n",
      "epoch: 202, loss: 6.285660266876221\n",
      "epoch: 203, loss: 6.239776134490967\n",
      "epoch: 204, loss: 6.194319725036621\n",
      "epoch: 205, loss: 6.149286270141602\n",
      "epoch: 206, loss: 6.104672908782959\n",
      "epoch: 207, loss: 6.060474395751953\n",
      "epoch: 208, loss: 6.016687870025635\n",
      "epoch: 209, loss: 5.973310470581055\n",
      "epoch: 210, loss: 5.9303364753723145\n",
      "epoch: 211, loss: 5.887762546539307\n",
      "epoch: 212, loss: 5.845584869384766\n",
      "epoch: 213, loss: 5.803800106048584\n",
      "epoch: 214, loss: 5.7624053955078125\n",
      "epoch: 215, loss: 5.721395015716553\n",
      "epoch: 216, loss: 5.68076753616333\n",
      "epoch: 217, loss: 5.640517711639404\n",
      "epoch: 218, loss: 5.600642204284668\n",
      "epoch: 219, loss: 5.561139106750488\n",
      "epoch: 220, loss: 5.522003173828125\n",
      "epoch: 221, loss: 5.483232021331787\n",
      "epoch: 222, loss: 5.444821834564209\n",
      "epoch: 223, loss: 5.406768321990967\n",
      "epoch: 224, loss: 5.369070053100586\n",
      "epoch: 225, loss: 5.331721782684326\n",
      "epoch: 226, loss: 5.294722080230713\n",
      "epoch: 227, loss: 5.258065700531006\n",
      "epoch: 228, loss: 5.2217512130737305\n",
      "epoch: 229, loss: 5.185774326324463\n",
      "epoch: 230, loss: 5.15013313293457\n",
      "epoch: 231, loss: 5.1148223876953125\n",
      "epoch: 232, loss: 5.079840183258057\n",
      "epoch: 233, loss: 5.045185089111328\n",
      "epoch: 234, loss: 5.01085090637207\n",
      "epoch: 235, loss: 4.97683572769165\n",
      "epoch: 236, loss: 4.943138122558594\n",
      "epoch: 237, loss: 4.909754276275635\n",
      "epoch: 238, loss: 4.87667989730835\n",
      "epoch: 239, loss: 4.843914031982422\n",
      "epoch: 240, loss: 4.811452388763428\n",
      "epoch: 241, loss: 4.779292583465576\n",
      "epoch: 242, loss: 4.747431755065918\n",
      "epoch: 243, loss: 4.715867519378662\n",
      "epoch: 244, loss: 4.684596538543701\n",
      "epoch: 245, loss: 4.653616428375244\n",
      "epoch: 246, loss: 4.6229248046875\n",
      "epoch: 247, loss: 4.592517375946045\n",
      "epoch: 248, loss: 4.562394142150879\n",
      "epoch: 249, loss: 4.5325493812561035\n",
      "epoch: 250, loss: 4.502983093261719\n",
      "epoch: 251, loss: 4.473690986633301\n",
      "epoch: 252, loss: 4.444672107696533\n",
      "epoch: 253, loss: 4.415922164916992\n",
      "epoch: 254, loss: 4.387439727783203\n",
      "epoch: 255, loss: 4.359221458435059\n",
      "epoch: 256, loss: 4.331265449523926\n",
      "epoch: 257, loss: 4.303569316864014\n",
      "epoch: 258, loss: 4.276130676269531\n",
      "epoch: 259, loss: 4.248946189880371\n",
      "epoch: 260, loss: 4.222014904022217\n",
      "epoch: 261, loss: 4.195333480834961\n",
      "epoch: 262, loss: 4.168900012969971\n",
      "epoch: 263, loss: 4.142712593078613\n",
      "epoch: 264, loss: 4.116766929626465\n",
      "epoch: 265, loss: 4.091063022613525\n",
      "epoch: 266, loss: 4.065598011016846\n",
      "epoch: 267, loss: 4.040369033813477\n",
      "epoch: 268, loss: 4.015374183654785\n",
      "epoch: 269, loss: 3.9906117916107178\n",
      "epoch: 270, loss: 3.966078996658325\n",
      "epoch: 271, loss: 3.9417734146118164\n",
      "epoch: 272, loss: 3.917693853378296\n",
      "epoch: 273, loss: 3.8938372135162354\n",
      "epoch: 274, loss: 3.8702027797698975\n",
      "epoch: 275, loss: 3.8467869758605957\n",
      "epoch: 276, loss: 3.8235886096954346\n",
      "epoch: 277, loss: 3.800605535507202\n",
      "epoch: 278, loss: 3.7778353691101074\n",
      "epoch: 279, loss: 3.755276918411255\n",
      "epoch: 280, loss: 3.7329273223876953\n",
      "epoch: 281, loss: 3.710785388946533\n",
      "epoch: 282, loss: 3.6888480186462402\n",
      "epoch: 283, loss: 3.667114496231079\n",
      "epoch: 284, loss: 3.645582437515259\n",
      "epoch: 285, loss: 3.6242501735687256\n",
      "epoch: 286, loss: 3.6031153202056885\n",
      "epoch: 287, loss: 3.582176923751831\n",
      "epoch: 288, loss: 3.561432123184204\n",
      "epoch: 289, loss: 3.540879249572754\n",
      "epoch: 290, loss: 3.520517110824585\n",
      "epoch: 291, loss: 3.5003437995910645\n",
      "epoch: 292, loss: 3.4803574085235596\n",
      "epoch: 293, loss: 3.4605562686920166\n",
      "epoch: 294, loss: 3.4409379959106445\n",
      "epoch: 295, loss: 3.421501874923706\n",
      "epoch: 296, loss: 3.4022457599639893\n",
      "epoch: 297, loss: 3.3831677436828613\n",
      "epoch: 298, loss: 3.3642661571502686\n",
      "epoch: 299, loss: 3.3455398082733154\n",
      "epoch: 300, loss: 3.326986789703369\n",
      "epoch: 301, loss: 3.308605909347534\n",
      "epoch: 302, loss: 3.2903945446014404\n",
      "epoch: 303, loss: 3.2723517417907715\n",
      "epoch: 304, loss: 3.2544760704040527\n",
      "epoch: 305, loss: 3.2367658615112305\n",
      "epoch: 306, loss: 3.219219207763672\n",
      "epoch: 307, loss: 3.2018349170684814\n",
      "epoch: 308, loss: 3.1846117973327637\n",
      "epoch: 309, loss: 3.1675477027893066\n",
      "epoch: 310, loss: 3.150641441345215\n",
      "epoch: 311, loss: 3.1338911056518555\n",
      "epoch: 312, loss: 3.1172964572906494\n",
      "epoch: 313, loss: 3.1008541584014893\n",
      "epoch: 314, loss: 3.084564685821533\n",
      "epoch: 315, loss: 3.068425416946411\n",
      "epoch: 316, loss: 3.0524353981018066\n",
      "epoch: 317, loss: 3.036593198776245\n",
      "epoch: 318, loss: 3.020897150039673\n",
      "epoch: 319, loss: 3.0053460597991943\n",
      "epoch: 320, loss: 2.989938735961914\n",
      "epoch: 321, loss: 2.9746737480163574\n",
      "epoch: 322, loss: 2.95954966545105\n",
      "epoch: 323, loss: 2.9445652961730957\n",
      "epoch: 324, loss: 2.9297187328338623\n",
      "epoch: 325, loss: 2.9150097370147705\n",
      "epoch: 326, loss: 2.9004364013671875\n",
      "epoch: 327, loss: 2.8859975337982178\n",
      "epoch: 328, loss: 2.871691942214966\n",
      "epoch: 329, loss: 2.8575186729431152\n",
      "epoch: 330, loss: 2.843475580215454\n",
      "epoch: 331, loss: 2.829561948776245\n",
      "epoch: 332, loss: 2.815776824951172\n",
      "epoch: 333, loss: 2.8021187782287598\n",
      "epoch: 334, loss: 2.7885868549346924\n",
      "epoch: 335, loss: 2.775179624557495\n",
      "epoch: 336, loss: 2.7618958950042725\n",
      "epoch: 337, loss: 2.748734474182129\n",
      "epoch: 338, loss: 2.735694408416748\n",
      "epoch: 339, loss: 2.7227745056152344\n",
      "epoch: 340, loss: 2.7099733352661133\n",
      "epoch: 341, loss: 2.6972901821136475\n",
      "epoch: 342, loss: 2.6847240924835205\n",
      "epoch: 343, loss: 2.6722733974456787\n",
      "epoch: 344, loss: 2.659937620162964\n",
      "epoch: 345, loss: 2.6477150917053223\n",
      "epoch: 346, loss: 2.635605573654175\n",
      "epoch: 347, loss: 2.6236066818237305\n",
      "epoch: 348, loss: 2.6117188930511475\n",
      "epoch: 349, loss: 2.599940061569214\n",
      "epoch: 350, loss: 2.5882697105407715\n",
      "epoch: 351, loss: 2.576706647872925\n",
      "epoch: 352, loss: 2.5652499198913574\n",
      "epoch: 353, loss: 2.553898572921753\n",
      "epoch: 354, loss: 2.542651414871216\n",
      "epoch: 355, loss: 2.531507968902588\n",
      "epoch: 356, loss: 2.5204663276672363\n",
      "epoch: 357, loss: 2.5095267295837402\n",
      "epoch: 358, loss: 2.4986870288848877\n",
      "epoch: 359, loss: 2.4879472255706787\n",
      "epoch: 360, loss: 2.477306365966797\n",
      "epoch: 361, loss: 2.4667625427246094\n",
      "epoch: 362, loss: 2.456315517425537\n",
      "epoch: 363, loss: 2.4459643363952637\n",
      "epoch: 364, loss: 2.43570876121521\n",
      "epoch: 365, loss: 2.425546884536743\n",
      "epoch: 366, loss: 2.415478229522705\n",
      "epoch: 367, loss: 2.4055018424987793\n",
      "epoch: 368, loss: 2.3956170082092285\n",
      "epoch: 369, loss: 2.3858225345611572\n",
      "epoch: 370, loss: 2.3761181831359863\n",
      "epoch: 371, loss: 2.366502523422241\n",
      "epoch: 372, loss: 2.3569750785827637\n",
      "epoch: 373, loss: 2.3475351333618164\n",
      "epoch: 374, loss: 2.338181257247925\n",
      "epoch: 375, loss: 2.328913450241089\n",
      "epoch: 376, loss: 2.319730043411255\n",
      "epoch: 377, loss: 2.3106305599212646\n",
      "epoch: 378, loss: 2.301614761352539\n",
      "epoch: 379, loss: 2.2926814556121826\n",
      "epoch: 380, loss: 2.283829689025879\n",
      "epoch: 381, loss: 2.2750585079193115\n",
      "epoch: 382, loss: 2.2663681507110596\n",
      "epoch: 383, loss: 2.2577569484710693\n",
      "epoch: 384, loss: 2.2492244243621826\n",
      "epoch: 385, loss: 2.240769863128662\n",
      "epoch: 386, loss: 2.2323925495147705\n",
      "epoch: 387, loss: 2.2240917682647705\n",
      "epoch: 388, loss: 2.215866804122925\n",
      "epoch: 389, loss: 2.207716703414917\n",
      "epoch: 390, loss: 2.199641466140747\n",
      "epoch: 391, loss: 2.1916394233703613\n",
      "epoch: 392, loss: 2.1837105751037598\n",
      "epoch: 393, loss: 2.175854206085205\n",
      "epoch: 394, loss: 2.1680688858032227\n",
      "epoch: 395, loss: 2.16035532951355\n",
      "epoch: 396, loss: 2.1527113914489746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 397, loss: 2.145137310028076\n",
      "epoch: 398, loss: 2.1376326084136963\n",
      "epoch: 399, loss: 2.1301958560943604\n",
      "epoch: 400, loss: 2.12282657623291\n",
      "epoch: 401, loss: 2.115525007247925\n",
      "epoch: 402, loss: 2.1082892417907715\n",
      "epoch: 403, loss: 2.1011202335357666\n",
      "epoch: 404, loss: 2.094015598297119\n",
      "epoch: 405, loss: 2.086975574493408\n",
      "epoch: 406, loss: 2.080000162124634\n",
      "epoch: 407, loss: 2.073087215423584\n",
      "epoch: 408, loss: 2.066237688064575\n",
      "epoch: 409, loss: 2.059450149536133\n",
      "epoch: 410, loss: 2.0527243614196777\n",
      "epoch: 411, loss: 2.0460598468780518\n",
      "epoch: 412, loss: 2.0394556522369385\n",
      "epoch: 413, loss: 2.0329113006591797\n",
      "epoch: 414, loss: 2.026426076889038\n",
      "epoch: 415, loss: 2.0199999809265137\n",
      "epoch: 416, loss: 2.013632297515869\n",
      "epoch: 417, loss: 2.007322072982788\n",
      "epoch: 418, loss: 2.0010688304901123\n",
      "epoch: 419, loss: 1.9948724508285522\n",
      "epoch: 420, loss: 1.9887322187423706\n",
      "epoch: 421, loss: 1.9826472997665405\n",
      "epoch: 422, loss: 1.976617693901062\n",
      "epoch: 423, loss: 1.9706425666809082\n",
      "epoch: 424, loss: 1.964721441268921\n",
      "epoch: 425, loss: 1.9588539600372314\n",
      "epoch: 426, loss: 1.953039288520813\n",
      "epoch: 427, loss: 1.947277307510376\n",
      "epoch: 428, loss: 1.941567301750183\n",
      "epoch: 429, loss: 1.9359087944030762\n",
      "epoch: 430, loss: 1.9303017854690552\n",
      "epoch: 431, loss: 1.924744725227356\n",
      "epoch: 432, loss: 1.9192382097244263\n",
      "epoch: 433, loss: 1.9137811660766602\n",
      "epoch: 434, loss: 1.9083733558654785\n",
      "epoch: 435, loss: 1.9030141830444336\n",
      "epoch: 436, loss: 1.8977032899856567\n",
      "epoch: 437, loss: 1.8924404382705688\n",
      "epoch: 438, loss: 1.8872246742248535\n",
      "epoch: 439, loss: 1.8820558786392212\n",
      "epoch: 440, loss: 1.8769336938858032\n",
      "epoch: 441, loss: 1.8718574047088623\n",
      "epoch: 442, loss: 1.8668268918991089\n",
      "epoch: 443, loss: 1.8618414402008057\n",
      "epoch: 444, loss: 1.856900930404663\n",
      "epoch: 445, loss: 1.852004885673523\n",
      "epoch: 446, loss: 1.8471524715423584\n",
      "epoch: 447, loss: 1.8423439264297485\n",
      "epoch: 448, loss: 1.837578296661377\n",
      "epoch: 449, loss: 1.832855463027954\n",
      "epoch: 450, loss: 1.8281747102737427\n",
      "epoch: 451, loss: 1.8235360383987427\n",
      "epoch: 452, loss: 1.8189388513565063\n",
      "epoch: 453, loss: 1.8143829107284546\n",
      "epoch: 454, loss: 1.8098676204681396\n",
      "epoch: 455, loss: 1.8053927421569824\n",
      "epoch: 456, loss: 1.8009576797485352\n",
      "epoch: 457, loss: 1.7965625524520874\n",
      "epoch: 458, loss: 1.7922066450119019\n",
      "epoch: 459, loss: 1.7878894805908203\n",
      "epoch: 460, loss: 1.7836107015609741\n",
      "epoch: 461, loss: 1.7793705463409424\n",
      "epoch: 462, loss: 1.7751678228378296\n",
      "epoch: 463, loss: 1.7710027694702148\n",
      "epoch: 464, loss: 1.7668746709823608\n",
      "epoch: 465, loss: 1.762783408164978\n",
      "epoch: 466, loss: 1.7587285041809082\n",
      "epoch: 467, loss: 1.7547096014022827\n",
      "epoch: 468, loss: 1.750726342201233\n",
      "epoch: 469, loss: 1.7467786073684692\n",
      "epoch: 470, loss: 1.7428659200668335\n",
      "epoch: 471, loss: 1.7389881610870361\n",
      "epoch: 472, loss: 1.7351446151733398\n",
      "epoch: 473, loss: 1.731335163116455\n",
      "epoch: 474, loss: 1.7275598049163818\n",
      "epoch: 475, loss: 1.7238175868988037\n",
      "epoch: 476, loss: 1.7201085090637207\n",
      "epoch: 477, loss: 1.7164324522018433\n",
      "epoch: 478, loss: 1.7127889394760132\n",
      "epoch: 479, loss: 1.7091777324676514\n",
      "epoch: 480, loss: 1.705597996711731\n",
      "epoch: 481, loss: 1.7020502090454102\n",
      "epoch: 482, loss: 1.6985336542129517\n",
      "epoch: 483, loss: 1.695048213005066\n",
      "epoch: 484, loss: 1.6915934085845947\n",
      "epoch: 485, loss: 1.6881693601608276\n",
      "epoch: 486, loss: 1.6847749948501587\n",
      "epoch: 487, loss: 1.681410789489746\n",
      "epoch: 488, loss: 1.678076148033142\n",
      "epoch: 489, loss: 1.6747708320617676\n",
      "epoch: 490, loss: 1.671494722366333\n",
      "epoch: 491, loss: 1.6682474613189697\n",
      "epoch: 492, loss: 1.6650283336639404\n",
      "epoch: 493, loss: 1.6618378162384033\n",
      "epoch: 494, loss: 1.658675193786621\n",
      "epoch: 495, loss: 1.6555402278900146\n",
      "epoch: 496, loss: 1.652432918548584\n",
      "epoch: 497, loss: 1.6493526697158813\n",
      "epoch: 498, loss: 1.6462993621826172\n",
      "epoch: 499, loss: 1.6432727575302124\n",
      "epoch: 500, loss: 1.640272617340088\n",
      "epoch: 501, loss: 1.637298822402954\n",
      "epoch: 502, loss: 1.6343508958816528\n",
      "epoch: 503, loss: 1.6314284801483154\n",
      "epoch: 504, loss: 1.628531575202942\n",
      "epoch: 505, loss: 1.6256603002548218\n",
      "epoch: 506, loss: 1.6228137016296387\n",
      "epoch: 507, loss: 1.6199920177459717\n",
      "epoch: 508, loss: 1.6171950101852417\n",
      "epoch: 509, loss: 1.6144219636917114\n",
      "epoch: 510, loss: 1.6116732358932495\n",
      "epoch: 511, loss: 1.6089483499526978\n",
      "epoch: 512, loss: 1.606246829032898\n",
      "epoch: 513, loss: 1.6035690307617188\n",
      "epoch: 514, loss: 1.6009143590927124\n",
      "epoch: 515, loss: 1.5982824563980103\n",
      "epoch: 516, loss: 1.5956735610961914\n",
      "epoch: 517, loss: 1.593087077140808\n",
      "epoch: 518, loss: 1.59052312374115\n",
      "epoch: 519, loss: 1.587981104850769\n",
      "epoch: 520, loss: 1.585461139678955\n",
      "epoch: 521, loss: 1.582962989807129\n",
      "epoch: 522, loss: 1.5804860591888428\n",
      "epoch: 523, loss: 1.5780307054519653\n",
      "epoch: 524, loss: 1.5755964517593384\n",
      "epoch: 525, loss: 1.5731828212738037\n",
      "epoch: 526, loss: 1.5707902908325195\n",
      "epoch: 527, loss: 1.568418025970459\n",
      "epoch: 528, loss: 1.5660662651062012\n",
      "epoch: 529, loss: 1.563734531402588\n",
      "epoch: 530, loss: 1.5614228248596191\n",
      "epoch: 531, loss: 1.5591309070587158\n",
      "epoch: 532, loss: 1.5568585395812988\n",
      "epoch: 533, loss: 1.5546056032180786\n",
      "epoch: 534, loss: 1.5523719787597656\n",
      "epoch: 535, loss: 1.5501571893692017\n",
      "epoch: 536, loss: 1.5479615926742554\n",
      "epoch: 537, loss: 1.5457842350006104\n",
      "epoch: 538, loss: 1.5436255931854248\n",
      "epoch: 539, loss: 1.54148530960083\n",
      "epoch: 540, loss: 1.5393632650375366\n",
      "epoch: 541, loss: 1.5372591018676758\n",
      "epoch: 542, loss: 1.535172939300537\n",
      "epoch: 543, loss: 1.533104419708252\n",
      "epoch: 544, loss: 1.5310533046722412\n",
      "epoch: 545, loss: 1.5290195941925049\n",
      "epoch: 546, loss: 1.5270030498504639\n",
      "epoch: 547, loss: 1.5250033140182495\n",
      "epoch: 548, loss: 1.523020625114441\n",
      "epoch: 549, loss: 1.5210546255111694\n",
      "epoch: 550, loss: 1.519105315208435\n",
      "epoch: 551, loss: 1.5171719789505005\n",
      "epoch: 552, loss: 1.5152552127838135\n",
      "epoch: 553, loss: 1.5133544206619263\n",
      "epoch: 554, loss: 1.5114694833755493\n",
      "epoch: 555, loss: 1.5096006393432617\n",
      "epoch: 556, loss: 1.5077470541000366\n",
      "epoch: 557, loss: 1.5059090852737427\n",
      "epoch: 558, loss: 1.5040864944458008\n",
      "epoch: 559, loss: 1.5022791624069214\n",
      "epoch: 560, loss: 1.500486969947815\n",
      "epoch: 561, loss: 1.4987095594406128\n",
      "epoch: 562, loss: 1.4969470500946045\n",
      "epoch: 563, loss: 1.4951990842819214\n",
      "epoch: 564, loss: 1.4934656620025635\n",
      "epoch: 565, loss: 1.4917463064193726\n",
      "epoch: 566, loss: 1.4900414943695068\n",
      "epoch: 567, loss: 1.488350749015808\n",
      "epoch: 568, loss: 1.4866739511489868\n",
      "epoch: 569, loss: 1.4850108623504639\n",
      "epoch: 570, loss: 1.4833617210388184\n",
      "epoch: 571, loss: 1.481726050376892\n",
      "epoch: 572, loss: 1.4801037311553955\n",
      "epoch: 573, loss: 1.4784952402114868\n",
      "epoch: 574, loss: 1.476899266242981\n",
      "epoch: 575, loss: 1.4753168821334839\n",
      "epoch: 576, loss: 1.4737471342086792\n",
      "epoch: 577, loss: 1.472190499305725\n",
      "epoch: 578, loss: 1.4706462621688843\n",
      "epoch: 579, loss: 1.469114899635315\n",
      "epoch: 580, loss: 1.4675958156585693\n",
      "epoch: 581, loss: 1.466089129447937\n",
      "epoch: 582, loss: 1.4645949602127075\n",
      "epoch: 583, loss: 1.4630376100540161\n",
      "epoch: 584, loss: 1.4568990468978882\n",
      "epoch: 585, loss: 1.4695005416870117\n",
      "epoch: 586, loss: 1.4609036445617676\n",
      "epoch: 587, loss: 1.4594494104385376\n",
      "epoch: 588, loss: 1.4580074548721313\n",
      "epoch: 589, loss: 1.4565768241882324\n",
      "epoch: 590, loss: 1.45515775680542\n",
      "epoch: 591, loss: 1.4537502527236938\n",
      "epoch: 592, loss: 1.452353835105896\n",
      "epoch: 593, loss: 1.4509689807891846\n",
      "epoch: 594, loss: 1.4495949745178223\n",
      "epoch: 595, loss: 1.4482320547103882\n",
      "epoch: 596, loss: 1.4468802213668823\n",
      "epoch: 597, loss: 1.4455389976501465\n",
      "epoch: 598, loss: 1.4442083835601807\n",
      "epoch: 599, loss: 1.4428884983062744\n",
      "epoch: 600, loss: 1.4415791034698486\n",
      "epoch: 601, loss: 1.4402800798416138\n",
      "epoch: 602, loss: 1.4389915466308594\n",
      "epoch: 603, loss: 1.4377131462097168\n",
      "epoch: 604, loss: 1.436444640159607\n",
      "epoch: 605, loss: 1.4351862668991089\n",
      "epoch: 606, loss: 1.433937907218933\n",
      "epoch: 607, loss: 1.43269944190979\n",
      "epoch: 608, loss: 1.4314708709716797\n",
      "epoch: 609, loss: 1.4302515983581543\n",
      "epoch: 610, loss: 1.4290422201156616\n",
      "epoch: 611, loss: 1.427842140197754\n",
      "epoch: 612, loss: 1.4266513586044312\n",
      "epoch: 613, loss: 1.4254701137542725\n",
      "epoch: 614, loss: 1.424213171005249\n",
      "epoch: 615, loss: 1.4197007417678833\n",
      "epoch: 616, loss: 1.4604412317276\n",
      "epoch: 617, loss: 1.4259532690048218\n",
      "epoch: 618, loss: 1.4247715473175049\n",
      "epoch: 619, loss: 1.4235992431640625\n",
      "epoch: 620, loss: 1.4224357604980469\n",
      "epoch: 621, loss: 1.4212815761566162\n",
      "epoch: 622, loss: 1.4201364517211914\n",
      "epoch: 623, loss: 1.4189999103546143\n",
      "epoch: 624, loss: 1.4178723096847534\n",
      "epoch: 625, loss: 1.4167535305023193\n",
      "epoch: 626, loss: 1.415643334388733\n",
      "epoch: 627, loss: 1.4145416021347046\n",
      "epoch: 628, loss: 1.4134482145309448\n",
      "epoch: 629, loss: 1.4123635292053223\n",
      "epoch: 630, loss: 1.4112869501113892\n",
      "epoch: 631, loss: 1.4102187156677246\n",
      "epoch: 632, loss: 1.409158706665039\n",
      "epoch: 633, loss: 1.4081065654754639\n",
      "epoch: 634, loss: 1.4070626497268677\n",
      "epoch: 635, loss: 1.4060264825820923\n",
      "epoch: 636, loss: 1.4049983024597168\n",
      "epoch: 637, loss: 1.403977870941162\n",
      "epoch: 638, loss: 1.4029653072357178\n",
      "epoch: 639, loss: 1.4019601345062256\n",
      "epoch: 640, loss: 1.4009627103805542\n",
      "epoch: 641, loss: 1.399972915649414\n",
      "epoch: 642, loss: 1.3989901542663574\n",
      "epoch: 643, loss: 1.398015022277832\n",
      "epoch: 644, loss: 1.3970471620559692\n",
      "epoch: 645, loss: 1.3960864543914795\n",
      "epoch: 646, loss: 1.3951330184936523\n",
      "epoch: 647, loss: 1.3941867351531982\n",
      "epoch: 648, loss: 1.3932470083236694\n",
      "epoch: 649, loss: 1.3923146724700928\n",
      "epoch: 650, loss: 1.39138925075531\n",
      "epoch: 651, loss: 1.3904706239700317\n",
      "epoch: 652, loss: 1.3895585536956787\n",
      "epoch: 653, loss: 1.3886533975601196\n",
      "epoch: 654, loss: 1.3877546787261963\n",
      "epoch: 655, loss: 1.3868627548217773\n",
      "epoch: 656, loss: 1.3859772682189941\n",
      "epoch: 657, loss: 1.3850980997085571\n",
      "epoch: 658, loss: 1.384225606918335\n",
      "epoch: 659, loss: 1.3833593130111694\n",
      "epoch: 660, loss: 1.3824992179870605\n",
      "epoch: 661, loss: 1.3816455602645874\n",
      "epoch: 662, loss: 1.3807977437973022\n",
      "epoch: 663, loss: 1.3799563646316528\n",
      "epoch: 664, loss: 1.3791210651397705\n",
      "epoch: 665, loss: 1.378291368484497\n",
      "epoch: 666, loss: 1.3774677515029907\n",
      "epoch: 667, loss: 1.376650094985962\n",
      "epoch: 668, loss: 1.375838279724121\n",
      "epoch: 669, loss: 1.3750320672988892\n",
      "epoch: 670, loss: 1.3742316961288452\n",
      "epoch: 671, loss: 1.3734371662139893\n",
      "epoch: 672, loss: 1.372531771659851\n",
      "epoch: 673, loss: 1.3831602334976196\n",
      "epoch: 674, loss: 1.375266432762146\n",
      "epoch: 675, loss: 1.3744562864303589\n",
      "epoch: 676, loss: 1.3736517429351807\n",
      "epoch: 677, loss: 1.3728529214859009\n",
      "epoch: 678, loss: 1.372059941291809\n",
      "epoch: 679, loss: 1.3712724447250366\n",
      "epoch: 680, loss: 1.370490550994873\n",
      "epoch: 681, loss: 1.3697141408920288\n",
      "epoch: 682, loss: 1.3689430952072144\n",
      "epoch: 683, loss: 1.3681775331497192\n",
      "epoch: 684, loss: 1.3674172163009644\n",
      "epoch: 685, loss: 1.3666622638702393\n",
      "epoch: 686, loss: 1.3659124374389648\n",
      "epoch: 687, loss: 1.3651678562164307\n",
      "epoch: 688, loss: 1.3644286394119263\n",
      "epoch: 689, loss: 1.3636943101882935\n",
      "epoch: 690, loss: 1.3629651069641113\n",
      "epoch: 691, loss: 1.3622407913208008\n",
      "epoch: 692, loss: 1.361521601676941\n",
      "epoch: 693, loss: 1.360807180404663\n",
      "epoch: 694, loss: 1.3600977659225464\n",
      "epoch: 695, loss: 1.3593931198120117\n",
      "epoch: 696, loss: 1.3586931228637695\n",
      "epoch: 697, loss: 1.3579981327056885\n",
      "epoch: 698, loss: 1.3573074340820312\n",
      "epoch: 699, loss: 1.3566216230392456\n",
      "epoch: 700, loss: 1.3559404611587524\n",
      "epoch: 701, loss: 1.3552638292312622\n",
      "epoch: 702, loss: 1.354591727256775\n",
      "epoch: 703, loss: 1.353924036026001\n",
      "epoch: 704, loss: 1.35326087474823\n",
      "epoch: 705, loss: 1.3526017665863037\n",
      "epoch: 706, loss: 1.3519471883773804\n",
      "epoch: 707, loss: 1.3512970209121704\n",
      "epoch: 708, loss: 1.3506512641906738\n",
      "epoch: 709, loss: 1.3500093221664429\n",
      "epoch: 710, loss: 1.3493719100952148\n",
      "epoch: 711, loss: 1.348738431930542\n",
      "epoch: 712, loss: 1.3481093645095825\n",
      "epoch: 713, loss: 1.3474841117858887\n",
      "epoch: 714, loss: 1.3468629121780396\n",
      "epoch: 715, loss: 1.3462458848953247\n",
      "epoch: 716, loss: 1.3456326723098755\n",
      "epoch: 717, loss: 1.3450233936309814\n",
      "epoch: 718, loss: 1.344417929649353\n",
      "epoch: 719, loss: 1.3438166379928589\n",
      "epoch: 720, loss: 1.3432189226150513\n",
      "epoch: 721, loss: 1.3426250219345093\n",
      "epoch: 722, loss: 1.3420348167419434\n",
      "epoch: 723, loss: 1.3414480686187744\n",
      "epoch: 724, loss: 1.3408656120300293\n",
      "epoch: 725, loss: 1.340286374092102\n",
      "epoch: 726, loss: 1.3397108316421509\n",
      "epoch: 727, loss: 1.3391388654708862\n",
      "epoch: 728, loss: 1.3385705947875977\n",
      "epoch: 729, loss: 1.338005781173706\n",
      "epoch: 730, loss: 1.3374443054199219\n",
      "epoch: 731, loss: 1.3368864059448242\n",
      "epoch: 732, loss: 1.3363317251205444\n",
      "epoch: 733, loss: 1.3357806205749512\n",
      "epoch: 734, loss: 1.3352326154708862\n",
      "epoch: 735, loss: 1.3346881866455078\n",
      "epoch: 736, loss: 1.3341470956802368\n",
      "epoch: 737, loss: 1.333608865737915\n",
      "epoch: 738, loss: 1.3330743312835693\n",
      "epoch: 739, loss: 1.3325426578521729\n",
      "epoch: 740, loss: 1.3320143222808838\n",
      "epoch: 741, loss: 1.3314892053604126\n",
      "epoch: 742, loss: 1.3309669494628906\n",
      "epoch: 743, loss: 1.330447793006897\n",
      "epoch: 744, loss: 1.3299319744110107\n",
      "epoch: 745, loss: 1.3294190168380737\n",
      "epoch: 746, loss: 1.3289088010787964\n",
      "epoch: 747, loss: 1.328401803970337\n",
      "epoch: 748, loss: 1.3278979063034058\n",
      "epoch: 749, loss: 1.3273968696594238\n",
      "epoch: 750, loss: 1.3268985748291016\n",
      "epoch: 751, loss: 1.3264031410217285\n",
      "epoch: 752, loss: 1.3259105682373047\n",
      "epoch: 753, loss: 1.3254207372665405\n",
      "epoch: 754, loss: 1.3249337673187256\n",
      "epoch: 755, loss: 1.3244495391845703\n",
      "epoch: 756, loss: 1.3239680528640747\n",
      "epoch: 757, loss: 1.3234890699386597\n",
      "epoch: 758, loss: 1.3230133056640625\n",
      "epoch: 759, loss: 1.3225396871566772\n",
      "epoch: 760, loss: 1.3220688104629517\n",
      "epoch: 761, loss: 1.3216004371643066\n",
      "epoch: 762, loss: 1.3211349248886108\n",
      "epoch: 763, loss: 1.320671558380127\n",
      "epoch: 764, loss: 1.3202112913131714\n",
      "epoch: 765, loss: 1.3197532892227173\n",
      "epoch: 766, loss: 1.3192977905273438\n",
      "epoch: 767, loss: 1.3188444375991821\n",
      "epoch: 768, loss: 1.3183938264846802\n",
      "epoch: 769, loss: 1.3179455995559692\n",
      "epoch: 770, loss: 1.3174997568130493\n",
      "epoch: 771, loss: 1.3170561790466309\n",
      "epoch: 772, loss: 1.316615104675293\n",
      "epoch: 773, loss: 1.316176414489746\n",
      "epoch: 774, loss: 1.3157398700714111\n",
      "epoch: 775, loss: 1.3153057098388672\n",
      "epoch: 776, loss: 1.3148738145828247\n",
      "epoch: 777, loss: 1.3144443035125732\n",
      "epoch: 778, loss: 1.3140169382095337\n",
      "epoch: 779, loss: 1.3135915994644165\n",
      "epoch: 780, loss: 1.3131684064865112\n",
      "epoch: 781, loss: 1.3127474784851074\n",
      "epoch: 782, loss: 1.3123286962509155\n",
      "epoch: 783, loss: 1.311911940574646\n",
      "epoch: 784, loss: 1.3114973306655884\n",
      "epoch: 785, loss: 1.3110852241516113\n",
      "epoch: 786, loss: 1.3106746673583984\n",
      "epoch: 787, loss: 1.3102664947509766\n",
      "epoch: 788, loss: 1.309860110282898\n",
      "epoch: 789, loss: 1.3094557523727417\n",
      "epoch: 790, loss: 1.3090534210205078\n",
      "epoch: 791, loss: 1.3086529970169067\n",
      "epoch: 792, loss: 1.3082547187805176\n",
      "epoch: 793, loss: 1.3078583478927612\n",
      "epoch: 794, loss: 1.3073865175247192\n",
      "epoch: 795, loss: 1.301348328590393\n",
      "epoch: 796, loss: 1.4038957357406616\n",
      "epoch: 797, loss: 1.3189802169799805\n",
      "epoch: 798, loss: 1.3184783458709717\n",
      "epoch: 799, loss: 1.3179795742034912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 800, loss: 1.31748366355896\n",
      "epoch: 801, loss: 1.3168535232543945\n",
      "epoch: 802, loss: 1.308613657951355\n",
      "epoch: 803, loss: 1.316056489944458\n",
      "epoch: 804, loss: 1.315571665763855\n",
      "epoch: 805, loss: 1.3150861263275146\n",
      "epoch: 806, loss: 1.307017207145691\n",
      "epoch: 807, loss: 1.3153187036514282\n",
      "epoch: 808, loss: 1.3148338794708252\n",
      "epoch: 809, loss: 1.314352035522461\n",
      "epoch: 810, loss: 1.3138726949691772\n",
      "epoch: 811, loss: 1.3133959770202637\n",
      "epoch: 812, loss: 1.3129221200942993\n",
      "epoch: 813, loss: 1.312450885772705\n",
      "epoch: 814, loss: 1.311982274055481\n",
      "epoch: 815, loss: 1.311516284942627\n",
      "epoch: 816, loss: 1.3110527992248535\n",
      "epoch: 817, loss: 1.3105919361114502\n",
      "epoch: 818, loss: 1.3101333379745483\n",
      "epoch: 819, loss: 1.3096774816513062\n",
      "epoch: 820, loss: 1.3092241287231445\n",
      "epoch: 821, loss: 1.308773159980774\n",
      "epoch: 822, loss: 1.3083246946334839\n",
      "epoch: 823, loss: 1.3078784942626953\n",
      "epoch: 824, loss: 1.3074346780776978\n",
      "epoch: 825, loss: 1.3069936037063599\n",
      "epoch: 826, loss: 1.3065544366836548\n",
      "epoch: 827, loss: 1.3061177730560303\n",
      "epoch: 828, loss: 1.3056833744049072\n",
      "epoch: 829, loss: 1.3052513599395752\n",
      "epoch: 830, loss: 1.3048213720321655\n",
      "epoch: 831, loss: 1.3043938875198364\n",
      "epoch: 832, loss: 1.3039685487747192\n",
      "epoch: 833, loss: 1.3035454750061035\n",
      "epoch: 834, loss: 1.3031243085861206\n",
      "epoch: 835, loss: 1.3027055263519287\n",
      "epoch: 836, loss: 1.3022887706756592\n",
      "epoch: 837, loss: 1.3018741607666016\n",
      "epoch: 838, loss: 1.3014618158340454\n",
      "epoch: 839, loss: 1.3010512590408325\n",
      "epoch: 840, loss: 1.3006430864334106\n",
      "epoch: 841, loss: 1.3002368211746216\n",
      "epoch: 842, loss: 1.2998323440551758\n",
      "epoch: 843, loss: 1.299430251121521\n",
      "epoch: 844, loss: 1.2990299463272095\n",
      "epoch: 845, loss: 1.2986315488815308\n",
      "epoch: 846, loss: 1.2982351779937744\n",
      "epoch: 847, loss: 1.2978408336639404\n",
      "epoch: 848, loss: 1.2974482774734497\n",
      "epoch: 849, loss: 1.2970575094223022\n",
      "epoch: 850, loss: 1.2966688871383667\n",
      "epoch: 851, loss: 1.2962819337844849\n",
      "epoch: 852, loss: 1.2958968877792358\n",
      "epoch: 853, loss: 1.29551362991333\n",
      "epoch: 854, loss: 1.2951321601867676\n",
      "epoch: 855, loss: 1.294752597808838\n",
      "epoch: 856, loss: 1.2943748235702515\n",
      "epoch: 857, loss: 1.2939985990524292\n",
      "epoch: 858, loss: 1.2936242818832397\n",
      "epoch: 859, loss: 1.2932052612304688\n",
      "epoch: 860, loss: 1.2845922708511353\n",
      "epoch: 861, loss: 1.2934625148773193\n",
      "epoch: 862, loss: 1.2930865287780762\n",
      "epoch: 863, loss: 1.2927120923995972\n",
      "epoch: 864, loss: 1.2923396825790405\n",
      "epoch: 865, loss: 1.291968822479248\n",
      "epoch: 866, loss: 1.2915996313095093\n",
      "epoch: 867, loss: 1.2912322282791138\n",
      "epoch: 868, loss: 1.2908663749694824\n",
      "epoch: 869, loss: 1.2905023097991943\n",
      "epoch: 870, loss: 1.2901397943496704\n",
      "epoch: 871, loss: 1.2897789478302002\n",
      "epoch: 872, loss: 1.2894196510314941\n",
      "epoch: 873, loss: 1.2890620231628418\n",
      "epoch: 874, loss: 1.288705825805664\n",
      "epoch: 875, loss: 1.2883514165878296\n",
      "epoch: 876, loss: 1.2879984378814697\n",
      "epoch: 877, loss: 1.2876468896865845\n",
      "epoch: 878, loss: 1.2872968912124634\n",
      "epoch: 879, loss: 1.286948561668396\n",
      "epoch: 880, loss: 1.2866015434265137\n",
      "epoch: 881, loss: 1.2862560749053955\n",
      "epoch: 882, loss: 1.2859121561050415\n",
      "epoch: 883, loss: 1.2855695486068726\n",
      "epoch: 884, loss: 1.2852283716201782\n",
      "epoch: 885, loss: 1.2848886251449585\n",
      "epoch: 886, loss: 1.2845503091812134\n",
      "epoch: 887, loss: 1.2842134237289429\n",
      "epoch: 888, loss: 1.283877968788147\n",
      "epoch: 889, loss: 1.2835438251495361\n",
      "epoch: 890, loss: 1.2832109928131104\n",
      "epoch: 891, loss: 1.2828795909881592\n",
      "epoch: 892, loss: 1.282549500465393\n",
      "epoch: 893, loss: 1.2822206020355225\n",
      "epoch: 894, loss: 1.281893253326416\n",
      "epoch: 895, loss: 1.281567096710205\n",
      "epoch: 896, loss: 1.2812422513961792\n",
      "epoch: 897, loss: 1.2809187173843384\n",
      "epoch: 898, loss: 1.280596375465393\n",
      "epoch: 899, loss: 1.2802751064300537\n",
      "epoch: 900, loss: 1.279955267906189\n",
      "epoch: 901, loss: 1.2796367406845093\n",
      "epoch: 902, loss: 1.2793192863464355\n",
      "epoch: 903, loss: 1.2790031433105469\n",
      "epoch: 904, loss: 1.2786659002304077\n",
      "epoch: 905, loss: 1.2702205181121826\n",
      "epoch: 906, loss: 1.279096007347107\n",
      "epoch: 907, loss: 1.2787752151489258\n",
      "epoch: 908, loss: 1.2784557342529297\n",
      "epoch: 909, loss: 1.2781375646591187\n",
      "epoch: 910, loss: 1.2778204679489136\n",
      "epoch: 911, loss: 1.2775046825408936\n",
      "epoch: 912, loss: 1.2771899700164795\n",
      "epoch: 913, loss: 1.2768765687942505\n",
      "epoch: 914, loss: 1.276564359664917\n",
      "epoch: 915, loss: 1.2762531042099\n",
      "epoch: 916, loss: 1.2759431600570679\n",
      "epoch: 917, loss: 1.2756341695785522\n",
      "epoch: 918, loss: 1.2753267288208008\n",
      "epoch: 919, loss: 1.2750201225280762\n",
      "epoch: 920, loss: 1.2747145891189575\n",
      "epoch: 921, loss: 1.2744102478027344\n",
      "epoch: 922, loss: 1.2741069793701172\n",
      "epoch: 923, loss: 1.273804783821106\n",
      "epoch: 924, loss: 1.2735035419464111\n",
      "epoch: 925, loss: 1.2732034921646118\n",
      "epoch: 926, loss: 1.2729045152664185\n",
      "epoch: 927, loss: 1.2726064920425415\n",
      "epoch: 928, loss: 1.272309422492981\n",
      "epoch: 929, loss: 1.2720136642456055\n",
      "epoch: 930, loss: 1.2717187404632568\n",
      "epoch: 931, loss: 1.2714247703552246\n",
      "epoch: 932, loss: 1.2711318731307983\n",
      "epoch: 933, loss: 1.2708399295806885\n",
      "epoch: 934, loss: 1.270548939704895\n",
      "epoch: 935, loss: 1.2702590227127075\n",
      "epoch: 936, loss: 1.2699700593948364\n",
      "epoch: 937, loss: 1.2696819305419922\n",
      "epoch: 938, loss: 1.2693946361541748\n",
      "epoch: 939, loss: 1.269108533859253\n",
      "epoch: 940, loss: 1.268823266029358\n",
      "epoch: 941, loss: 1.2685389518737793\n",
      "epoch: 942, loss: 1.268255591392517\n",
      "epoch: 943, loss: 1.2679729461669922\n",
      "epoch: 944, loss: 1.2676911354064941\n",
      "epoch: 945, loss: 1.267410397529602\n",
      "epoch: 946, loss: 1.2671304941177368\n",
      "epoch: 947, loss: 1.266851544380188\n",
      "epoch: 948, loss: 1.2665733098983765\n",
      "epoch: 949, loss: 1.2662959098815918\n",
      "epoch: 950, loss: 1.266019582748413\n",
      "epoch: 951, loss: 1.2657439708709717\n",
      "epoch: 952, loss: 1.2653744220733643\n",
      "epoch: 953, loss: 1.256784200668335\n",
      "epoch: 954, loss: 1.2658824920654297\n",
      "epoch: 955, loss: 1.265601634979248\n",
      "epoch: 956, loss: 1.2653216123580933\n",
      "epoch: 957, loss: 1.2650424242019653\n",
      "epoch: 958, loss: 1.2647639513015747\n",
      "epoch: 959, loss: 1.26448655128479\n",
      "epoch: 960, loss: 1.2642097473144531\n",
      "epoch: 961, loss: 1.2639338970184326\n",
      "epoch: 962, loss: 1.263659119606018\n",
      "epoch: 963, loss: 1.2633848190307617\n",
      "epoch: 964, loss: 1.2631115913391113\n",
      "epoch: 965, loss: 1.2628389596939087\n",
      "epoch: 966, loss: 1.2625672817230225\n",
      "epoch: 967, loss: 1.2622963190078735\n",
      "epoch: 968, loss: 1.2620261907577515\n",
      "epoch: 969, loss: 1.2617568969726562\n",
      "epoch: 970, loss: 1.2614881992340088\n",
      "epoch: 971, loss: 1.2612204551696777\n",
      "epoch: 972, loss: 1.260953426361084\n",
      "epoch: 973, loss: 1.260686993598938\n",
      "epoch: 974, loss: 1.2604215145111084\n",
      "epoch: 975, loss: 1.2601567506790161\n",
      "epoch: 976, loss: 1.2598927021026611\n",
      "epoch: 977, loss: 1.259629249572754\n",
      "epoch: 978, loss: 1.259366750717163\n",
      "epoch: 979, loss: 1.25910484790802\n",
      "epoch: 980, loss: 1.2588435411453247\n",
      "epoch: 981, loss: 1.2585829496383667\n",
      "epoch: 982, loss: 1.258323311805725\n",
      "epoch: 983, loss: 1.2580642700195312\n",
      "epoch: 984, loss: 1.2578057050704956\n",
      "epoch: 985, loss: 1.2575480937957764\n",
      "epoch: 986, loss: 1.2572908401489258\n",
      "epoch: 987, loss: 1.257034420967102\n",
      "epoch: 988, loss: 1.2567787170410156\n",
      "epoch: 989, loss: 1.256523609161377\n",
      "epoch: 990, loss: 1.256269097328186\n",
      "epoch: 991, loss: 1.2560153007507324\n",
      "epoch: 992, loss: 1.2557621002197266\n",
      "epoch: 993, loss: 1.2555094957351685\n",
      "epoch: 994, loss: 1.2552577257156372\n",
      "epoch: 995, loss: 1.255006194114685\n",
      "epoch: 996, loss: 1.2547554969787598\n",
      "epoch: 997, loss: 1.2543542385101318\n",
      "epoch: 998, loss: 1.245750904083252\n",
      "epoch: 999, loss: 1.2549747228622437\n"
     ]
    }
   ],
   "source": [
    "# former method: directly training without wrapping in class\n",
    "\n",
    "losses = []\n",
    "shift_model = nn.Sequential()\n",
    "shift_model.add_module(\"dense1\", nn.Linear(61, 65))\n",
    "shift_model.add_module(\"act1\", nn.LeakyReLU())\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer_shifts = torch.optim.SGD(shift_model.parameters(), lr=1500)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    shifts_pred = shift_model(phonemes)\n",
    "    loss_shift = loss_fn(shifts_pred,shifts)\n",
    "    losses.append(loss_shift)\n",
    "    print(f\"epoch: {epoch}, loss: {loss_shift}\")\n",
    "    optimizer_shifts.zero_grad()\n",
    "    loss_shift.backward()\n",
    "    optimizer_shifts.step()\n",
    "    \n",
    "#     if len(losses)>10:\n",
    "#         if losses[-1]-losses[-5]>=0:\n",
    "#             print(\"detecting minimum validation loss. stopping early\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "103b7486",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.4583,  -1.0995,  -0.1913,  ..., -11.5097, -14.5831, -17.9243],\n",
       "        [ -3.4450,  -1.0836,  -0.1787,  ..., -11.3884, -14.4382, -17.7612],\n",
       "        [ -3.4554,  -1.0850,  -0.1708,  ..., -11.3925, -14.4398, -17.8874],\n",
       "        ...,\n",
       "        [ -3.4883,  -1.1132,  -0.1923,  ..., -11.4141, -14.4596, -17.7866],\n",
       "        [ -3.5228,  -1.1621,  -0.2488,  ..., -11.2006, -14.2407, -17.5608],\n",
       "        [ -3.4557,  -1.0935,  -0.1879,  ..., -11.2472, -14.2974, -17.7299]],\n",
       "       grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifts_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "946db38c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.8344,  -0.4313,   0.4293,  ..., -11.9093, -15.0479, -18.3271],\n",
       "        [ -2.6625,  -0.2299,   0.6109,  ..., -11.3789, -14.4855, -17.8279],\n",
       "        [ -2.7991,  -0.2632,   0.7154,  ..., -11.4353, -14.5021, -17.8652],\n",
       "        ...,\n",
       "        [ -3.2440,  -0.6030,   0.4282,  ..., -11.7126, -14.7787, -18.1606],\n",
       "        [ -3.7102,  -1.3226,  -0.3567,  ...,  -8.7786, -11.7740, -15.0684],\n",
       "        [ -2.8030,  -0.3818,   0.4941,  ...,  -9.4130, -12.5467, -15.8897]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a10272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zmuv_normalize_phoneme_class(feature_matrix, calculate_mean_and_std=True, phoneme_mean=None, phoneme_std=None):\n",
    "    '''\n",
    "    return the normalized feature class of matrix phoneme_class_features_matrix, the mean and standard deviations\n",
    "    inputs:\n",
    "        feature_matrix (np.array): a 2d feature matrix of various instances of a phoneme. The phonemes instances are stacked on the z (?) axis\n",
    "        calculate_mean_and_std (bool): whether to calculate mean and std or not. should be false if inputting values for phoneme_mean and phoneme_std\n",
    "        phoneme_mean (np.array):  the average feature\n",
    "        phoneme_std (np.array):  the standard deviation of features\n",
    "        \n",
    "    returns:\n",
    "        normalized_features (np.array): the normalized feature matrix \n",
    "        phoneme_mean (np.array):  the average feature\n",
    "        phoneme_std (np.array):  the standard deviation of features\n",
    "    '''\n",
    "    if calculate_mean_and_std:\n",
    "        phoneme_mean, phoneme_std = np.mean(feature_matrix,0),np.std(feature_matrix,0)\n",
    "    normalized_features = (feature_matrix - phoneme_mean)/(phoneme_std)\n",
    "    \n",
    "    return normalized_features, phoneme_mean, phoneme_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phonemes_from_single_file(filename):\n",
    "    '''\n",
    "    this returns a dicitonary mapping phoneme names to numpy 1-D matrices of features of the phoneme class of a particular length (padded with nans if data is less than) found in a file\n",
    "    inputs:\n",
    "        filename (str): the input file\n",
    "        \n",
    "    '''\n",
    "    output_dictionary = {}\n",
    "    f = open(filename, \"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    f.close()\n",
    "    for line in lines: \n",
    "        line = line.split()\n",
    "        current_phoneme = line[-1]\n",
    "        line = line[:-1]\n",
    "        line = np.array(line,dtype=\"float64\")\n",
    "        if current_phoneme in output_dictionary:\n",
    "            output_dictionary[current_phoneme].append(line)\n",
    "        else:\n",
    "            output_dictionary[current_phoneme] = [line]\n",
    "        \n",
    "    return output_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing above function\n",
    "TEST_FILE = 'Y:/personal/ojuba.mezisashe/Sentences/TIMIT_dp/output/TRAIN/DR1/FCJF0/SI648.txt'\n",
    "print(TEST_FILE)\n",
    "phonemes_from_file = get_phonemes_from_single_file(TEST_FILE)\n",
    "# _ = get_phonemes_from_single_file(filename, 100) # edge case test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme = \"aa\"\n",
    "phonemes_from_file[phoneme][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "d = np.diag(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e832a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4624d6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  6, 12, 20])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*a+a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73759f41",
   "metadata": {},
   "source": [
    "## Combined Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the PyTorch Geometric model\n",
    "class GNN(torch_geometric.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, mlp):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = torch_geometric.nn.GCNConv(input_size, hidden_size)\n",
    "        self.conv2 = torch_geometric.nn.GCNConv(hidden_size, output_size)\n",
    "        self.mlp = mlp\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "# Define the combined model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, mlp, gnn):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.mlp = mlp\n",
    "        self.gnn = gnn\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply the MLP to the feature vectors\n",
    "        x = self.mlp(x)\n",
    "        # Apply the GNN to the graph and feature vectors\n",
    "        x = self.gnn(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Create instances of the PyTorch and PyTorch Geometric models\n",
    "mlp = MLP(input_size=10, hidden_size=20, output_size=5)\n",
    "gnn = GNN(input_size=20, hidden_size=30, output_size=10, mlp=mlp)\n",
    "\n",
    "# Create an instance of the combined model\n",
    "combined_model = CombinedModel(mlp, gnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
