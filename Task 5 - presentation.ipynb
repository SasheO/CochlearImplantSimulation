{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788e8ded",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b2e00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_extraction import read_feat_file\n",
    "import os\n",
    "from conf import read_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752b601",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_sets = [\"test_hint_office_0_1_3\", \"test_hint_lecture_0_1_3\", \"test_hint_stairway_0_1_3_90\"]\n",
    "conf_file_filename = \"LSTMModelTransform_presentation2.txt\"\n",
    "conf_file = \"conf/fft_mask_dp_fft/\"+conf_file_filename\n",
    "conf_dict = read_conf(conf_file)\n",
    "model_name = \"model1\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c1c4f",
   "metadata": {},
   "source": [
    "# Visualize features before and after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4db1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from importlib.metadata import *\n",
    "except ImportError:  # Python < 3.10 (backport)\n",
    "    from importlib_metadata import *\n",
    "import umap.umap_ as umap\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf736b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from train import get_device\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from feature_extraction import TorchStandardScaler\n",
    "import numpy as np\n",
    "from file_loader_model_transform import read_feat_list,encode_phones\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from net import get_model_type\n",
    "# import loss_fn\n",
    "# Labels\n",
    "from phone_mapping import get_label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba88b85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_umap(features, phonemes_from, phoneme_to_number, diagram_name, colour_map=None):\n",
    "    \n",
    "    '''\n",
    "    this is a modification of plot_umap in ../../visualizations/plot_umap.py\n",
    "    plots a umap vizual\n",
    "\n",
    "    input:\n",
    "        features (2D np.array): the features  extracted using feature_extraction/extractTrainingData or feature_extraction/extractTestingData. Each row represents a feature sample.\n",
    "        phonemes_from (list or 1D np.array): the phoneme number the features correspond to. each value corresponds to a row in features.\n",
    "        phoneme_to_number (dict): maps phoneme names to numbers that represent it in phonemes_from\n",
    "        diagram_name (str): name of the output visual\n",
    "        colour_map (dict): maps label of phoneme classification groups to a set/list of phonemes in that group. for example, the manner of articulation colour_map (from https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4100697) would be {\"vowels\":{\"iy\",\"ih\", 'eh', 'ae', 'aa', 'ah', 'ao', 'uh', 'uw', 'ux', 'ax', 'ax-h', 'ix'},                             \"dipthongs\":{'ey', 'aw', 'ay', 'oy', 'ow'},\"semi-vowels\": {'l', 'el', 'r', 'w', 'y', 'er', 'axr'},\"stops\": {'b', 'd', 'g', 'p', 't', 'k', 'jh', 'ch'},\"fricatives\": {'s', 'sh', 'z', 'zh', 'f', 'th', 'v', 'dh', 'hh', 'hv'},\"nasals\": {'m', 'em', 'n', 'nx', 'ng', 'eng', 'en'},\"silence\": {'dx', 'bcl', 'dcl', 'gcl', 'pcl', 'tcl', 'kcl', 'h', 'pau', 'epi', 'q'},\"h#\": {\"h#\"}} \n",
    "    '''\n",
    "    \n",
    "    features = np.array(features, dtype=float)\n",
    "    features = np.nan_to_num(features, copy=True, posinf=0, neginf=0)\n",
    "    features = features[:,:65] # since only the first 65 as per kevin's feature extraction code are teh actual features and the rest are ideal mask values\n",
    "    phonemes_from = phonemes_from\n",
    "    print(len(phonemes_from))\n",
    "    # return\n",
    "    # copy pasted most of this code from mnist example\n",
    "    reducer = umap.UMAP(random_state=42, low_memory=True)\n",
    "    embedding = reducer.fit_transform(features)\n",
    "\n",
    "    # if issues, may be due to nan values. think how to deal with sparse data? maybe cut off at some point.\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    colors = phonemes_from\n",
    "    if len(colors) < 5000:\n",
    "        s = 10\n",
    "    elif len(colors) < 100000:\n",
    "        s=3\n",
    "    else:\n",
    "        s =1\n",
    "\n",
    "    \n",
    "    \n",
    "    if colour_map:\n",
    "        number_to_phoneme = {}\n",
    "\n",
    "        for y,x in phoneme_to_number.items():\n",
    "            number_to_phoneme[x]=y\n",
    "        phonemes_from_colour_coded = []\n",
    "        for phoneme_num in phonemes_from:\n",
    "            phoneme = number_to_phoneme[phoneme_num]\n",
    "            added = False\n",
    "            for indx in range(len(colour_map)):\n",
    "                if phoneme in list(colour_map.items())[indx][1]:\n",
    "                    phonemes_from_colour_coded.append(indx)\n",
    "                    added = True\n",
    "                    break\n",
    "            if added:\n",
    "                continue\n",
    "            else:\n",
    "                phonemes_from_colour_coded.append(len(colour_map)+1)\n",
    "        colors = phonemes_from_colour_coded\n",
    "\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], c=colors, cmap=\"Spectral\", s=s)\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "\n",
    "    plt.title(diagram_name, fontsize=18)\n",
    "    \n",
    "    # TODO: show axis in matplotlib\n",
    "    # plt.axes.Axes.imshow()\n",
    "\n",
    "    if colour_map:\n",
    "        try:\n",
    "            plt.colorbar(boundaries=np.arange(len(set(colors)))+0.5).set_ticks(ticks=np.arange(len(set(colors))+1), labels=list(colour_map.keys())) #+[\"other\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            plt.title(e, fontsize=18)\n",
    "            m=plt.colorbar(boundaries=np.arange(len(set(colors))))\n",
    "            m.set_ticks(ticks=np.arange(len(set(colors)))) #+[\"other\"])\n",
    "            m.set_ticklabels(list(colour_map.keys()))\n",
    "    else:\n",
    "        plt.colorbar().ax.tick_params(labelsize=10)\n",
    "\n",
    "    output_image_num = 1\n",
    "    output_image = str(output_image_num)+\"_.png\"\n",
    "    while os.path.isfile(output_image):\n",
    "        output_image_num += 1\n",
    "        output_image = str(output_image_num)+\"_.png\"\n",
    "\n",
    "    plt.show(output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1312e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def transform_features(features, phonemes, phoneme_scales_map, phoneme_shifts_map):\n",
    "    transformed_features = []\n",
    "    for indx in range(len(features)):\n",
    "        feature = features[indx]\n",
    "        phoneme = phonemes[indx]\n",
    "        scale = phoneme_scales_map[phoneme]\n",
    "        shift = phoneme_shifts_map[phoneme]\n",
    "        transformed_feature = (feature * scale) + shift\n",
    "        transformed_features.append(transformed_features)\n",
    "    \n",
    "    return np.array(transformed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa035646",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get testing data features files list\n",
    "file_list = []\n",
    "\n",
    "for test_set in test_sets:\n",
    "    test_feat_list = \"data/\" + conf_dict[\"mask\"] + \"/\" + test_set + \"/\" + conf_dict[\"feature_type\"] + \".txt\"\n",
    "    file_list.extend(read_feat_list(test_feat_list))\n",
    "file_list = [x.replace('/run/user/1000/gvfs/smb-share:server=pse-fs-01.egr.duke.edu,share=lcollins-00/data/data/personal', \"Y:/personal\") for x in file_list] # do this so code works on my windows laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b478d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "le_bpg = get_label_encoder(conf_dict[\"bpg\"])\n",
    "phonemes_encoded = encode_phones(le_bpg.classes_, le_bpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449eb19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# colour_maps\n",
    "\n",
    "manner_of_articulation_map = {\"vowels\":{\"iy\",\"ih\", 'eh', 'ae', 'aa', 'ah', 'ao', 'uh', 'uw', 'ux', 'ax', 'ax-h', 'ix'},\n",
    "                             \"dipthongs\":{'ey', 'aw', 'ay', 'oy', 'ow'},\n",
    "                             \"semi-vowels\": {'l', 'el', 'r', 'w', 'y', 'er', 'axr'},\n",
    "                             \"stops\": {'b', 'd', 'g', 'p', 't', 'k', 'jh', 'ch'},\n",
    "                             \"fricatives\": {'s', 'sh', 'z', 'zh', 'f', 'th', 'v', 'dh', 'hh', 'hv'},\n",
    "                             \"nasals\": {'m', 'em', 'n', 'nx', 'ng', 'eng', 'en'},\n",
    "                             \"silence\": {'dx', 'bcl', 'dcl', 'gcl', 'pcl', 'tcl', 'kcl', 'h', 'pau', 'epi', 'q'},\n",
    "                             \"h#\": {\"h#\"}}\n",
    "\n",
    "vowels_and_consonants_map = {\"vowels\":{\"iy\",\"ih\", 'eh', 'ae', 'aa', 'ah', 'ao', 'uh', 'uw', 'ux', 'ax', 'ax-h', 'ix', 'ey', 'aw', 'ay', 'oy', 'ow'},\n",
    "                             \"semi-vowels\": {'l', 'el', 'r', 'w', 'y', 'er', 'axr'},\n",
    "                             \"consonants\": {'b', 'd', 'g', 'p', 't', 'k', 'jh', 'ch', 's', 'sh', 'z', 'zh', 'f', 'th', 'v', 'dh', 'hh', 'hv', 'm', 'em', 'n', 'nx', 'ng', 'eng', 'en', 'dx', 'bcl', 'dcl', 'gcl', 'pcl', 'tcl', 'kcl', 'h', 'pau', 'epi', 'q'},\n",
    "                             \"h#\": {\"h#\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347a737",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# run this if using arpanet phonemes (43) instead of timit phonemes (61)\n",
    "_vowels_and_consonants_map = {}\n",
    "_manner_of_articulation_map = {}\n",
    "\n",
    "\n",
    "for phoneme_class,phoneme_set in vowels_and_consonants_map.items():\n",
    "    _vowels_and_consonants_map[phoneme_class] = set()\n",
    "    for phoneme in phoneme_set:\n",
    "        if phoneme in le_bpg.classes_:\n",
    "            _vowels_and_consonants_map[phoneme_class].add(phoneme)\n",
    "\n",
    "\n",
    "for phoneme_class,phoneme_set in manner_of_articulation_map.items():\n",
    "    _manner_of_articulation_map[phoneme_class] = set()\n",
    "    for phoneme in phoneme_set:\n",
    "        if phoneme in le_bpg.classes_:\n",
    "            _manner_of_articulation_map[phoneme_class].add(phoneme)\n",
    "\n",
    "\n",
    "vowels_and_consonants_map = _vowels_and_consonants_map\n",
    "manner_of_articulation_map = _manner_of_articulation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db84c76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get the data\n",
    "features = []\n",
    "phonemes = []\n",
    "\n",
    "# for file in file_list: # read all 720 files\n",
    "for file in file_list[:5]: # sampling 5 files here for testing purposes\n",
    "    _features, _, _phonemes = read_feat_file(file, conf_dict)\n",
    "    features.extend(_features)\n",
    "    phonemes.extend(_phonemes)\n",
    "\n",
    "features = np.array(features)\n",
    "phonemes = np.array(phonemes)\n",
    "phonemes_from = []\n",
    "phoneme_to_number = {}\n",
    "\n",
    "for phoneme in phonemes:\n",
    "    if phoneme in phoneme_to_number:\n",
    "        pass\n",
    "    else:\n",
    "        phoneme_to_number[phoneme] = len(phoneme_to_number)+1\n",
    "    phonemes_from.append(phoneme_to_number[phoneme])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a135e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get and save transformation scale and shift values to ../../saved_variables/model_scales_and_weights if not already done\n",
    "'''\n",
    "steps:\n",
    "    load the model.\n",
    "    run each phoneme encoding through the model.scale_model, model.shift_model\n",
    "    save the outputs using np.savetxt to the ../../saved_variables folder in format {phoneme}_scale.txt or {phoneme}_shift.txt\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cea28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get the transformation scale and shift values if saved to ../../saved_variables/model_scales_and_weights\n",
    "dir_with_scale_and_shift_vectors = \"../../saved_variables/model_scales_and_weights\"\n",
    "scale_vectors = {}\n",
    "shift_vectors = {}\n",
    "end_indx = -len(\"_scale.txt\")\n",
    "\n",
    "for filename in os.listdir(dir_with_scale_and_shift_vectors):\n",
    "    phoneme = filename[:end_indx]\n",
    "    if \"scale\" in filename:\n",
    "        scale_file = dir_with_scale_and_shift_vectors+\"/\"+filename\n",
    "        shift_file = dir_with_scale_and_shift_vectors+\"/\"+phoneme+\"_shift.txt\"\n",
    "        phoneme = filename[:end_indx]\n",
    "        scale_vectors[phoneme] = np.loadtxt(scale_file)\n",
    "        shift_vectors[phoneme] = np.loadtxt(shift_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73aef0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get transformed features\n",
    "sample_num = 500\n",
    "transformed_features = transform_features(features[:sample_num], phonemes, scale_vectors, shift_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052c313",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# plot umap of features\n",
    "plot_umap(features[:sample_num], phonemes_from, phoneme_to_number, \"Phonemes Before Transformation\", manner_of_articulation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3bd51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# plot umap of transformed features\n",
    "plot_umap(transformed_features, phonemes_from, phoneme_to_number, \"Phonemes Before Transformation\", manner_of_articulation_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec61ce8",
   "metadata": {},
   "source": [
    "# Show STOI and SRMR performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34160e56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def read_objective_intelligibility_results(filename):\n",
    "    with open(filename,\"r\") as f:\n",
    "        objective_intelligibility_scores = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    objective_intelligibility_scores = [float(x.strip()) for x in objective_intelligibility_scores]\n",
    "    return objective_intelligibility_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346301ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metric = \"srmr\" # \"srmr\" or \"stoi\"\n",
    "\n",
    "metric_to_barplot_label = {\n",
    "    \"srmr\":\"SRMR-CI Intelligibility Score\",\n",
    "    \"stoi\":\"STOI Score\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299848c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "objective_intelligibility_dp_scores = []\n",
    "objective_intelligibility_ideal_scores = []\n",
    "objective_intelligibility_simplified_phoneme_dependent_scores = []\n",
    "objective_intelligibility_reverb_scores = []\n",
    "\n",
    "\n",
    "for test_set in test_sets:\n",
    "    results_dir = \"exp/\" + conf_dict[\"mask\"] +\"/\"+ conf_file_filename[:-4] +\"/\" + model_name + \"/results/\" + test_set\n",
    "    for filename in os.listdir(results_dir):\n",
    "        file = results_dir + \"/\" + filename\n",
    "        if metric in filename:\n",
    "            if metric == \"srmr\":\n",
    "                if \"_dp.txt\" in filename:\n",
    "                    objective_intelligibility_dp_scores.extend(read_objective_intelligibility_results(file))\n",
    "                elif \"estimated.txt\" in filename:\n",
    "                    objective_intelligibility_simplified_phoneme_dependent_scores.extend(read_objective_intelligibility_results(file))\n",
    "                elif \"reverb.txt\" in filename:\n",
    "                    objective_intelligibility_reverb_scores.extend(read_objective_intelligibility_results(file))\n",
    "                elif \"ideal.txt\" in filename:\n",
    "                    objective_intelligibility_ideal_scores.extend(read_objective_intelligibility_results(file))\n",
    "            elif metric == \"stoi\":\n",
    "                if \"ideal.txt\" in filename:\n",
    "                    objective_intelligibility_dp_scores.extend(read_objective_intelligibility_results(file))\n",
    "                elif \"estimated.txt\" in filename:\n",
    "                    objective_intelligibility_simplified_phoneme_dependent_scores.extend(read_objective_intelligibility_results(file))\n",
    "                elif \"reverb.txt\" in filename:\n",
    "                    objective_intelligibility_reverb_scores.extend(read_objective_intelligibility_results(file))\n",
    "                elif \"ibm.txt\" in filename:\n",
    "                    objective_intelligibility_ideal_scores.extend(read_objective_intelligibility_results(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f0792",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(objective_intelligibility_dp_scores)==len(objective_intelligibility_ideal_scores)==len(objective_intelligibility_simplified_phoneme_dependent_scores)==len(objective_intelligibility_reverb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68073b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def average(list_of_values):\n",
    "    return sum(list_of_values)/len(list_of_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d04479",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bars_to_values = {\"REV\":average(objective_intelligibility_reverb_scores),\n",
    "\"Simplified\\nPhoneme-\\nSpecific Model\":average(objective_intelligibility_simplified_phoneme_dependent_scores),\n",
    "\"Ideal Mask\":average(objective_intelligibility_ideal_scores),\n",
    "\"Direct Path\":average(objective_intelligibility_dp_scores)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f6a3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get the outputs from kevin's phoneme independent models\n",
    "objective_intelligibility_phoneme_independent_scores = []\n",
    "\n",
    "for test_set in test_sets:\n",
    "    results_dir = \"exp/\" + conf_dict[\"mask\"] +\"/LSTM_1layer_but_rev_log_fft_8kutts_batch16_sigapprox/model1/results/\" + test_set\n",
    "    for filename in os.listdir(results_dir):\n",
    "        file = results_dir + \"/\" + filename\n",
    "        if metric in filename:\n",
    "            if metric == \"srmr\":\n",
    "                if \"updated\" in filename: # because the way to calculate srmr has been updated and that's what i'm using\n",
    "                    pass \n",
    "                else:\n",
    "                    continue\n",
    "            if \"estimated.txt\" in filename:\n",
    "                objective_intelligibility_phoneme_independent_scores.extend(read_objective_intelligibility_results(file))\n",
    "\n",
    "bars_to_values[\"Phoneme-\\nIndependent\\nModel\"] = average(objective_intelligibility_phoneme_independent_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc1ebd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: get the ouput from kevin's moe model\n",
    "objective_intelligibility_moe_phoneme_dependent_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e6418",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded91de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9d8e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# creating the bar plot\n",
    "plt.bar(bars_to_values.keys(), bars_to_values.values(), color ='maroon', \n",
    "        width = 0.4)\n",
    "\n",
    "plt.ylabel(metric_to_barplot_label[metric])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
